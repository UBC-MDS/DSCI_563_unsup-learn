{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab1.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/563_lab_banner.png)\n",
    "\n",
    "# Lab 1: Clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports <a name=\"im\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option(\"display.max_colwidth\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "## Submission instructions <a name=\"si\"></a>\n",
    "rubric={mechanics}\n",
    "\n",
    "You will receive marks for correctly submitting this assignment by following the instructions below:\n",
    "    \n",
    "- Be sure to follow the [general lab instructions](https://ubc-mds.github.io/resources_pages/general_lab_instructions/).\n",
    "- [Here](https://github.com/UBC-MDS/public/tree/master/rubric) you will find the description of each rubric used in MDS.\n",
    "- Make at least three commits in your lab's GitHub repository.    \n",
    "- Push the final .ipynb file with your solutions to your GitHub repository for this lab.        \n",
    "- Before submitting your lab, run all cells in your notebook to make sure there are no errors by doing `Kernel -> Restart Kernel and Clear All Outputs` and then `Run -> Run All Cells`. Notebooks without the output displayed may not be graded at all (because we need to see the output in order to grade your work).     \n",
    "- Make sure to enroll to Gradescope via [Canvas](https://canvas.ubc.ca/courses/130310).\n",
    "- Upload the .ipynb file to Gradescope.\n",
    "- Make sure that your plots/output are rendered properly in Gradescope.    \n",
    "- If the .ipynb file is too big or doesn't render on Gradescope for some reason, also upload a pdf (preferably WebPDF) or html export of .ipynb file with your solutions so that TAs can view your submission on Gradescope. \n",
    "- The data you download for this lab <b>SHOULD NOT BE PUSHED TO YOUR REPOSITORY</b> (there is also a `.gitignore` in the repo to prevent this).\n",
    "- Include a clickable link to your GitHub repo for the lab just below this cell.\n",
    "</div>    \n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR REPO LINK GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Document clustering warm-up\n",
    "<hr>\n",
    "\n",
    "In the lectures, we explored image clustering. In this exercise, you will explore another popular application of clustering, [**document clustering**](https://en.wikipedia.org/wiki/Document_clustering). \n",
    "\n",
    "A large amount of unlabeled text data is available out there (e.g., news, recipes, online Q&A, and tweets). Clustering is a commonly used technique to organize this data in a meaningful way. \n",
    "\n",
    "As a warm up, in this exercise, you will cluster sentences from a toy corpus. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "Run the code below which \n",
    "- extracts content of Wikipedia articles on a set of queries and stores the first sentence in each article as a document representing that topic,\n",
    "- tokenizes the text (i.e., separates \"words\" in the sentence), and \n",
    "- carries out preliminary preprocessing (e.g., removes punctuation marks and converts the text to lower case).\n",
    "\n",
    "Some notes: \n",
    "- Typically, text preprocessing or normalization is an elaborate process before carrying out document clustering. But in this lab, we'll carry out minimal preprocessing, as we will be dealing with fairly short documents which are more or less clean.\n",
    "  \n",
    "- If you have created `conda` environment using [the course `yml` file](https://github.ubc.ca/mds-2023-24/DSCI_563_unsup-learn_students/blob/master/env-dsci-563.yml), you should have the following packages. If not, you may have to install appropriate packages in our course's environment.\n",
    "  \n",
    "- Feel free to experiment with Wikipedia queries of your choice. But stick to the provided list for the final submission so that it's easier for the TAs to grade your submission.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For tokenization we are using the `nltk` package. Even if you have the package installed via the course `conda` environment, you might have to download `nltk` pre-trained models, which can be done with the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wikipedia\n",
    "import string \n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "queries = [\n",
    "    \"Artificial Intelligence\", \"Deep learning\", \"Unsupervised learning\", \"Quantum Computing\", \n",
    "    \"Environmental protection\", \"Climate Change\", \"Renewable Energy\", \"Biodiversity\",\n",
    "    \"French Cuisine\", \"Bread food\", \"Dumpling food\", \"Pizza\"\n",
    "]\n",
    "\n",
    "wiki_dict = {\"wiki query\": [], \"text\": [], \"n_words\": []}\n",
    "remove_tokens = list(string.punctuation) + ['``', '’', '`', 'br', '\"', \"”\", \"''\", \"'s\", \"(\", \")\", \"[\", \"]\"]\n",
    "\n",
    "# Running this code might take some time.\n",
    "for query in queries:\n",
    "    try:\n",
    "        # Attempt to fetch the page content\n",
    "        page_content = wikipedia.page(query).content\n",
    "    except wikipedia.exceptions.PageError:\n",
    "        print(f\"Page not found for query: {query}. Skipping...\")\n",
    "        continue\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        print(f\"Query: {query} led to a disambiguation page. Choosing the first option: {e.options[0]}\")\n",
    "        page_content = wikipedia.page(e.options[0]).content\n",
    "\n",
    "    text = sent_tokenize(page_content)[0]\n",
    "    tokenized = word_tokenize(text)\n",
    "    text_pp = [token.lower() for token in tokenized if token.lower() not in remove_tokens]\n",
    "    wiki_dict[\"n_words\"].append(len(text_pp))\n",
    "    wiki_dict[\"text\"].append(\" \".join(text_pp))\n",
    "    wiki_dict[\"wiki query\"].append(query)\n",
    "\n",
    "wiki_df = pd.DataFrame(wiki_dict)\n",
    "wiki_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our toy corpus has seven toy documents (`text` column in `wiki_df`) extracted from seven Wikipedia queries (`wiki query` column in `wiki_df`). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.1 How many clusters? \n",
    "rubric={reasoning}\n",
    "\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "- If tasked with manually clustering documents from this toy corpus, how many clusters would you identify, and what labels would you assign to each cluster?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.2 K-Means with bag-of-words representation \n",
    "rubric={accuracy}\n",
    "\n",
    "In the lecture, we saw that data representation plays a crucial role in clustering. Changing flattened representation of images to feature vectors extracted from pre-trained models greatly improved the quality of clustering. \n",
    "\n",
    "What kind of representation is suitable for text data? In previous machine learning courses, we have used bag-of-words representation to numerically encode text data, where each document is represented with a vector of word frequencies. \n",
    "\n",
    "Let's try clustering documents with this simplistic representation.  \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Create bag-of-words representation using [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) with default arguments for the `text` column in `wiki_df` above.\n",
    "2. Cluster the encoded documents with [`KMeans` clustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html). Use `random_state=42` (for reproducibility) and set `n_clusters` to the number you identified in the previous exercise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kmeans_bow_labels = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wiki_df[\"bow_kmeans\"] = kmeans_bow_labels\n",
    "wiki_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.3 K-Means with sentence embedding representation\n",
    "rubric={accuracy}\n",
    "\n",
    "The bag-of-words representation, while useful, has limitations due to its inability to consider word order and context. There are other richer and more expressive representations of text which can be extracted using transfer learning. Similar to how pre-trained CNN models are employed for image data, there are pre-trained models for text data as well. In this lab, we will use a pre-trained model called 'all-MiniLM-L6-v2', accessible via the [sentence transformer](https://www.sbert.net/index.html) package. This deep learning model produces dense, fixed-length vector representations of sentences. For a comprehensive list of all available pre-trained models through this package, refer [here](https://www.sbert.net/docs/pretrained_models.html). These models are designed to capture the context and semantic meaning of sentences, making them particularly effective when we want to capture semantic similarity between texts. We may delve deeper into these representations in DSCI 575.\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Run the code below to create sentence embedding representation of documents in our toy corpus. \n",
    "2. Cluster documents in our toy corpus encoded with this representation (`emb_sents`) and `KMeans` with following arguments: \n",
    "    - `random_state=42` (for reproducibility)\n",
    "    - `n_clusters`=the number of clusters you identified in 1.1\n",
    "\n",
    "Note\n",
    "- The code below might throw a warning. You may ignore it for the purpose of this lab. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# embedder = SentenceTransformer(\"paraphrase-distilroberta-base-v1\")\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "emb_sents = embedder.encode(wiki_df[\"text\"].tolist())\n",
    "emb_sent_df = pd.DataFrame(emb_sents, index=wiki_df.index)\n",
    "emb_sent_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "kmeans_emb_labels = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wiki_df[\"emb_kmeans\"] = kmeans_emb_labels\n",
    "wiki_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.4 DBSCAN with sentence embedding representation and cosine distance  \n",
    "rubric={accuracy}\n",
    "\n",
    "Now try [`DBSCAN`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) on our toy dataset.\n",
    "K-Means clustering is inherently linked to Euclidean distance due to its reliance on the concept of means. \n",
    "With `DBSCAN` we have the flexibility to experiment with different distance metrics. For text data, [cosine similarities](https://scikit-learn.org/stable/modules/metrics.html#cosine-similarity) or cosine distances are often effective. The **cosine distance** between two vectors $u$ and $v$ is defined as: \n",
    "\n",
    "$$distance_{cosine}(u,v) = 1 - (\\frac{u \\cdot v}{\\left\\lVert u\\right\\rVert_2 \\left\\lVert v\\right\\rVert_2})$$\n",
    "\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "- Cluster documents in our toy corpus encoded with sentence embedding representation (`emb_sents`) and [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html?highlight=dbscan#sklearn.cluster.DBSCAN) with `metric='cosine'`. You will have to tune the hyperparamters `eps` and `min_samples` to get meaningful clusters, as default values of these hyperparameters are unlikely to work well on this toy dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.4\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dbscan_emb_labels = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wiki_df[\"emb_dbscan\"] = dbscan_emb_labels\n",
    "wiki_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.5 Hierarchical clustering with sentence embedding representation\n",
    "rubric={accuracy}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "Try hierarchical clustering on `emb_sents`. In particular\n",
    "1. Create and show a dendrogram with `complete` linkage and `metric='cosine'` on this toy dataset.\n",
    "2. Create flat clusters using `fcluster` with appropriate hyperparameters and store cluster labels to `hier_emb_labels` variable below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.5\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "hier_emb_labels = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wiki_df[\"emb_hierarchical\"] = hier_emb_labels\n",
    "wiki_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.6 Discussion\n",
    "rubric={reasoning}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "- Reflect on and discuss the clustering results of the methods you explored in the previous exercises, focusing on the following points:    \n",
    "    - effect of input representation on clustering results\n",
    "    - whether the clustering results match with your intuitions and the challenges associated with getting the desired clustering results with each method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1.6\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Gaussian Mixture Models (GMMs) \n",
    "\n",
    "In this exercise, you'll investigate Gaussian Mixture Models (GMMs) using a toy dataset. Take a look at the following dataset:\n",
    "\n",
    "How many clusters can you identify? While there's no definitive answer, it appears there could be 5 clusters. Three of these clusters seem to be blobs of points with roughly similar spreads. The other two clusters are elongated in shape and oriented in different directions. Moreover, they intersect, which could lead to ambiguous cluster assignments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/gmm-data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(df['X1'], df['X2'], alpha=0.6, edgecolors='k');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.1 How would K-Means behave?\n",
    "rubric={viz,accuracy}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "- Apply the K-Means clustering algorithm to the dataset with $k=3$ and $k=5$. Ensure you scale the data beforehand using [StandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html).\n",
    "- Visualize the data points, colouring them according to the cluster assignments. Display the plots for different $k$ values side by side.\n",
    "\n",
    "_You may use the visualization library of your choice._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2.1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.2 Clustering with Gaussian Mixture Model (GMM)\n",
    "rubric={viz,reasoning}\n",
    "\n",
    "**Your tasks:**\n",
    "- Fit a Gaussian Mixture Model (GMM) to the dataset using 5 components, experimenting with different values for the `covariance_type` argument. Use `random_state=42`, `max_iter=1000`, and `n_init=100`. \n",
    "- Visualize the data points by colouring them based on the cluster assignments from the GMM. In particular, create a 2-by-2 grid of plots, each coloured according to the cluster assignments, to illustrate the effect of each `covariance_type`. \n",
    "- Briefly describe how the different covariance types impact the shapes and orientations of the clusters.\n",
    "- Compare and contrast the best results with those you obtained using the K-Means algorithm in the previous exercise.\n",
    "\n",
    "_You may use the visualization library of your choice._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2.2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Clustering Flickr8k images and captions\n",
    "\n",
    "Now that you have experience with text and image clustering separately, this exercise introduces multimodal clustering. This approach involves integrating different types of data, in this case visual and textual, to improve the clustering results. The concept is based on the idea that by leveraging the complementary information provided by each modality, we can achieve improved clustering results.\n",
    "\n",
    "In the lectures, we discussed clustering images using pre-trained models as feature extractors, where these feature vectors represent the images. In Exercise 1 of this lab, you tackled document clustering using sentence embeddings derived from pre-trained language models. Expanding on these concepts, the current exercise focuses on creating a composite representation by merging image features extracted through pre-trained CNNs with caption features extracted from pre-trained language models.\n",
    "\n",
    "For this exercise, you'll use a subset of the [Flickr8k dataset](https://www.kaggle.com/datasets/adityajn105/flickr8k). The original dataset comprises 8,000 images. Each image is accompanied by five distinct captions that provide clear descriptions of the significant entities and events in the images. In the subset selected for this exercise, we will be working with approximately 400 images, and each image will be accompanied by one associated caption. \n",
    "\n",
    "Download this subset from [here](https://github.ubc.ca/mds-2021-22/datasets/blob/master/data/sampled_Flickr8k.zip), unzip it, and put the `sampled_Flickr8k` folder under the data directory in lab1. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the code below which reads images and corresponding captions in a list called `img_captions` and displays some sample images and corresponding captions from this list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torchvision.models import resnet50\n",
    "from torchvision import transforms, models, datasets\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import KMeans\n",
    "import random\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the device appropirately\n",
    "\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "data_dir = os.path.join(\"data\", \"sampled_Flickr8k\")  # Directory containing sampled images and captions \n",
    "sampled_img_path = os.path.join(data_dir, 'images')  # Directory containing sampled images\n",
    "sampled_captions_file = os.path.join(data_dir, 'sampled_captions.txt')  # Sampled captions file path\n",
    "\n",
    "# Function to load images and captions\n",
    "def load_images_and_captions(captions_file):    \n",
    "    data = {}\n",
    "    with open(captions_file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        img_id, caption = line.strip().split('%')\n",
    "        path = os.path.join(sampled_img_path, img_id)\n",
    "        data[path] = caption\n",
    "            \n",
    "    return list(data.items())\n",
    "    \n",
    "img_captions = load_images_and_captions(sampled_captions_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code snippet above reads sampled images along with their corresponding captions and stores them in `img_captions` as a list of (image_path, caption) tuples, as illustrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_captions[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's write code to display some sample images along with their captions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrap_caption(caption, max_length=40):\n",
    "    \"\"\"A simple function to wrap text based on a maximum line length.\"\"\"\n",
    "    words = caption.split()\n",
    "    wrapped_caption = \"\"\n",
    "    current_line = \"\"\n",
    "    for word in words:\n",
    "        if len(current_line) + len(word) + 1 <= max_length:\n",
    "            current_line += word + \" \"\n",
    "        else:\n",
    "            wrapped_caption += current_line.strip() + \"\\n\"\n",
    "            current_line = word + \" \"\n",
    "    wrapped_caption += current_line.strip()  # Add the last line\n",
    "    return wrapped_caption\n",
    "\n",
    "\n",
    "def display_samples(image_caption_pairs, n_samples = 5):\n",
    "    \"\"\"\n",
    "    Displays a random selection of image-caption pairs.\n",
    "\n",
    "    This function randomly selects a specified number of image-caption pairs\n",
    "    from a given list, resizes each image to a uniform size, and displays them\n",
    "    alongside their captions in a single row.\n",
    "\n",
    "    Parameters:\n",
    "    - image_caption_pairs (list of tuples): A list where each tuple contains\n",
    "      the path to an image (str) and its corresponding caption (str).\n",
    "    - n_samples (int, optional): The number of image-caption pairs to display.\n",
    "      Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "    - None. The function directly displays the images and captions using matplotlib.\n",
    "    \"\"\"    \n",
    "    sampled_items = random.sample(image_caption_pairs, n_samples)\n",
    "\n",
    "    desired_size = (200, 200)\n",
    "    \n",
    "    fig, axes = plt.subplots(1, n_samples, figsize=(20, 10), subplot_kw={\"xticks\": [], \"yticks\": []})\n",
    "    axes = axes.flatten()  # Flatten the 2D numpy array to easily iterate over it\n",
    "\n",
    "    for i, (img_path, caption) in enumerate(sampled_items):\n",
    "        ax = axes[i]\n",
    "        img = Image.open(img_path).convert('RGB')  # Open and convert to RGB\n",
    "        img = img.resize(desired_size)\n",
    "        ax.imshow(img)\n",
    "        ax.set_title(wrap_caption(caption), fontsize=12)\n",
    "        ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_samples(img_captions, n_samples = 5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 3.1 Clustering with image features\n",
    "rubric={accuracy}\n",
    "\n",
    "In this exercise, you will perform clustering on image features using the the pre-trained DenseNet CNN model. The code below \n",
    "- Loads the DenseNet model assuming that you've the appropriate device defined.\n",
    "- Extracts image features for all the images using the loaded DenseNet model and creates 1024-dimensional feature vectors for each image in our dataset.   \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "- Experiment with K-Means with different values for `n_clusters` on `image_features` extracted below.\n",
    "- Store cluster labels of your best model into `cluster_labels_img` and show sample images from each cluster using the provided code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "densenet = models.densenet121(weights='DenseNet121_Weights.DEFAULT')\n",
    "densenet.classifier = nn.Identity()  # remove that last \"classification\" layer\n",
    "densenet = densenet.to(device)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Running this code might take some time.\n",
    "def extract_image_features(model, img_captions):\n",
    "    model.eval()  # Ensure the model is in evaluation mode\n",
    "    features = []\n",
    "    with torch.no_grad():\n",
    "        for img_path, _ in img_captions:\n",
    "            image = Image.open(img_path).convert('RGB')\n",
    "            image = transform(image).unsqueeze(0).to(device)\n",
    "            feat_vec = model(image)\n",
    "            feat_vec = feat_vec.squeeze().cpu().numpy()  # Move to CPU and convert to NumPy\n",
    "            features.append(feat_vec)\n",
    "    return np.array(features)\n",
    "    \n",
    "image_features = extract_image_features(densenet, img_captions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "image_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_3.1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_clusters = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster_labels_img_feats = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def organize_and_print_clusters(img_captions, cluster_labels, n_clusters=5, n_samples=5):\n",
    "    \"\"\"\n",
    "    Organizes images and captions into specified clusters and displays samples from each cluster.\n",
    "\n",
    "    This function takes a list of image-caption pairs and their corresponding cluster labels,\n",
    "    organizes them into clusters, and displays a specified number of samples from each cluster\n",
    "    along with their captions.\n",
    "\n",
    "    Parameters:\n",
    "    - img_captions (list of tuples): A list where each tuple contains the path to an image (str) \n",
    "      and its corresponding caption (str).\n",
    "    - cluster_labels (list of int): A list of integer labels indicating the cluster assignment \n",
    "      for each image-caption pair in img_captions.\n",
    "    - n_clusters (int, optional): The number of clusters. Defaults to 5.\n",
    "    - n_samples (int, optional): The number of image-caption pairs to display from each cluster. \n",
    "      Defaults to 5.\n",
    "\n",
    "    Returns:\n",
    "    - None. This function prints the cluster ID and displays the images with captions for each cluster.\n",
    "\n",
    "    Notes:\n",
    "    - This function assumes that the `display_samples` function is defined and capable of displaying \n",
    "      image-caption pairs.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Organize images and captions into clusters based on cluster IDs \n",
    "    # in cluster_labels_text_feats\n",
    "    clustered = {i: [] for i in range(n_clusters)}\n",
    "    for item, label in zip(img_captions, cluster_labels):\n",
    "        clustered[label].append((item[0], item[1]))\n",
    "        \n",
    "    # Print cluster IDs and display 5 sample images with captions from each cluster\n",
    "    # created using text features    \n",
    "    for cluster in clustered: \n",
    "        print(f'\\n\\n\\n Cluster {cluster}')\n",
    "        display_samples(clustered[cluster], n_samples=n_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "organize_and_print_clusters(img_captions, cluster_labels_img_feats, n_clusters=n_clusters, n_samples=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 3.2 Clustering with caption features\n",
    "rubric={accuracy}\n",
    "\n",
    "In this exercise, you will perform clustering on captions using the pre-trained `all-MiniLM-L6-v2` language model. The provided code loads the `all-MiniLM-L6-v2` model. \n",
    "\n",
    "**Your taks:**\n",
    "- Use the pre-trained `all-MiniLM-L6-v2` model to encode captions.\n",
    "- Try different values for `n_clusters` in `K-Means` clustering with the encoded captions.\n",
    "- Save the cluster labels in `cluster_labels_text_feats`. Then, execute the provided code to display sample images and captions from each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Load sentence-transformer model\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_3.2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_clusters = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster_labels_text_feats = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "organize_and_print_clusters(img_captions, cluster_labels_text_feats, n_clusters=n_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 3.3 Integrating image and text features for multi-modal clustering\n",
    "rubric={accuracy}\n",
    "\n",
    "You may have observed that clustering based on image features highlights visual similarities, while clustering based on caption features groups examples with semantic resemblance in captions.\n",
    "\n",
    "In this exercise, you will combine:\n",
    "\n",
    "- Image features extracted from the pre-trained `DenseNet` CNN model.\n",
    "- Caption features extracted from the pre-trained `all-MiniLM-L6-v2` language model.\n",
    "\n",
    "You will then cluster examples using these combined representations.\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "- Combine the image and caption features obtained in the prior exercises so that each image caption pair is represented with a 1024 + 384 dimensional feature vector. \n",
    "- Normalize the combined features using `StandardScaler` to ensure both text and image features are on a comparable scale.\n",
    "- Experiment with different clustering methods, pick a clustering method of your choice, and apply it to the combined features. Experiment with different hyperparameter values, but only report your optimal results.\n",
    "- Store the cluster labels in `cluster_labels_combined`. Afterwards, run the provided code snippet to visualize sample images and captions from each cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_3.3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_clusters = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "cluster_labels_combined = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "organize_and_print_clusters(img_captions, cluster_labels_combined, n_clusters=n_clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 3.4 Discussion\n",
    "rubric={reasoning}\n",
    "\n",
    "- Compare and contrast your clustering results from 3.1, 3.2, and 3.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_3.4\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Food for thought\n",
    "<hr>\n",
    "\n",
    "Similar to the previous courses, each lab will have a few challenging questions. In some of the labs I will be including challenging questions which lead to the material in the upcoming week. These are usually low-risk questions and will contribute to maximum 5% of the lab grade. The main purpose here is to challenge yourself or dig deeper in a particular area. When you start working on labs, attempt all other questions before moving to these challenging questions. If you are running out of time, please skip the challenging questions. \n",
    "\n",
    "We will be more strict with the marking of these questions. There might not be model answers. If you want to get full points in these questions, your answers need to\n",
    "- be thorough, thoughtful, and well-written\n",
    "- provide convincing justification and appropriate evidence for the claims you make \n",
    "- impress the reader of your lab with your understanding of the material, your analytical and critical reasoning skills, and your ability to think on your own\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/eva-game-on.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### (Challenging) 4.1: Similarity measure for mixed datasets\n",
    "rubric={reasoning}\n",
    "\n",
    "Clustering is based on finding similar examples. So using appropriate similarity metric is crucial in order to find meaningful clusters. When the data contains only numeric features you can apply appropriate transformation (e.g., scaling) and find similarity between examples based on Euclidean distances between points. In document clustering with sentence embedding representation we used cosine similarity. But what if the dataset contains different types of features such as numeric, categorical (e.g., postal code), or multi-valued categorical features (e.g., movie genres)? How would you calculate similarity between examples as a single numeric value and apply clustering methods? Suggest some ideas.  \n",
    "\n",
    "> As a concrete example, you may explore clustering of movies from [the IMDB Movies Dataset](https://www.kaggle.com/datasets/harshitshankhdhar/imdb-dataset-of-top-1000-movies-and-tv-shows). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_4.1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### (Challenging) 4.2: Vector Quantization\n",
    "rubric={reasoning}\n",
    "\n",
    "One more application of clustering is _vector quantization_, where we find a prototype point for each cluster and replace points in the cluster by their prototype. If our inputs are images, vector quantization gives us a rudimentary image compression algorithm.\n",
    "\n",
    "We will implement image quantization by filling in the `quantize` and `dequantize` functions below. The `quantize` function should take in an image,  and using the pixels as examples and the 3 colour channels as features, run KMeans clustering on the data with $2^b$ clusters for some hyperparameter $b$. The code should store the cluster means and return the cluster assignments. The `dequantize` function should return a version of the image (the same size as the original) where each pixel's original colour is replaced with the nearest prototype colour.\n",
    "\n",
    "To understand why this is compression, consider the original image space. Say the image can take on the values $0,1,\\ldots,255$ in each colour channel. Since $2^8=256$ this means we need 8 bits to represent each colour channel, for a total of 24 bits per pixel. Using our method, we are restricting each pixel to only take on one of $2^b$ colour values. In other words, we are compressing each pixel from a 24-bit colour representation to a $b$-bit colour representation by picking the $2^b$ prototype colours that are \"most representative\" given the content of the image. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Complete the `quantize` and `dequantize` functions below.\n",
    "2. Run the code on an image of your choosing. Display the results for a few different values of $b$.\n",
    "\n",
    "Notes: \n",
    "- If you actually try saving this as a file, you won't see the file size being what you expected, because Python won't know to allocate exactly $b$ bits per element of `quantized_img`, and will instead probably store them as 32-bit integers if you don't specify otherwise. But if one wanted to work harder, it would be theoretically possible to store the elements with $b$ bits per pixel. Also, all this is before any additional lossless compression.\n",
    "\n",
    "- This is not how image compression systems like JPEG actually work. They use something similar to the Fourier transform, followed by a (simpler) quantization, followed by lossless compression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_4.2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from matplotlib.pyplot import imread, imshow\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img = imread(os.path.join(\"img/eva-happy-saturday.jpg\"))\n",
    "imshow(img)\n",
    "plt.title(\"original image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def quantize(img, b):\n",
    "    \"\"\"\n",
    "    Quantizes an image into 2^b clusters\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    img : a (H,W,3) numpy array\n",
    "      the image to be processed\n",
    "    b   : int\n",
    "      the desired number of bits\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    quantized_img : a (H,W) numpy array containing cluster indices\n",
    "    colours       : a (2^b, 3) numpy array, each row is a colour\n",
    "    \"\"\"\n",
    "\n",
    "    H, W, _ = img.shape\n",
    "    model = KMeans(n_clusters=2 ** b)\n",
    "\n",
    "    ### YOUR CODE\n",
    "    ...\n",
    "\n",
    "    return quantized_img, model.cluster_centers_.astype(\"uint8\")\n",
    "\n",
    "\n",
    "def dequantize(quantized_img, colours):\n",
    "    H, W = quantized_img.shape\n",
    "    img = np.zeros((H, W, 3), dtype=\"uint8\")\n",
    "\n",
    "    # YOUR CODE: fill in the values of `img` here\n",
    "\n",
    "    ...\n",
    "\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "img.reshape(398 * 398, 3).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "b = 3\n",
    "compressed, colours = quantize(img, b=b)\n",
    "recon = dequantize(compressed, colours)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 10))\n",
    "ax[0].imshow(img)\n",
    "ax[0].set_title(\"original image\")\n",
    "ax[1].imshow(recon)\n",
    "ax[1].set_title(f\"reconstructed image (b = {b})\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.imshow(colours[None])\n",
    "plt.title(\"colours learned\")\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before submitting your assignment, please make sure you have followed all the instructions in the Submission Instructions section at the top. \n",
    "\n",
    "Well done!! Have a great weekend and happy reading week! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "Image(\"img/eva-well-done.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "563",
   "language": "python",
   "name": "563"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
