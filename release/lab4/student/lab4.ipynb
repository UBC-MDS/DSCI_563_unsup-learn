{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab4.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/563_lab_banner.png)\n",
    "\n",
    "# Lab 4: Movie Recommendations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from hashlib import sha1\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "<div class=\"alert alert-info\">\n",
    "\n",
    "## Submission instructions <a name=\"si\"></a>\n",
    "rubric={mechanics}\n",
    "\n",
    "You will receive marks for correctly submitting this assignment by following the instructions below:\n",
    "    \n",
    "- Be sure to follow the [general lab instructions](https://ubc-mds.github.io/resources_pages/general_lab_instructions/).\n",
    "- [Here](https://github.com/UBC-MDS/public/tree/master/rubric) you will find the description of each rubric used in MDS.\n",
    "- Make at least three commits in your lab's GitHub repository.    \n",
    "- Push the final .ipynb file with your solutions to your GitHub repository for this lab.        \n",
    "- Before submitting your lab, run all cells in your notebook to make sure there are no errors by doing `Kernel -> Restart Kernel and Clear All Outputs` and then `Run -> Run All Cells`. Notebooks without the output displayed may not be graded at all (because we need to see the output in order to grade your work).     \n",
    "- Make sure to enroll to Gradescope via [Canvas](https://canvas.ubc.ca/courses/106525).\n",
    "- Upload the .ipynb file to Gradescope.\n",
    "- Make sure that your plots/output are rendered properly in Gradescope.    \n",
    "- If the .ipynb file is too big or doesn't render on Gradescope for some reason, also upload a pdf (preferably WebPDF) or html export of .ipynb file with your solutions so that TAs can view your submission on Gradescope. \n",
    "- The data you download for this lab <b>SHOULD NOT BE PUSHED TO YOUR REPOSITORY</b> (there is also a `.gitignore` in the repo to prevent this).\n",
    "- Include a clickable link to your GitHub repo for the lab just below this cell.\n",
    "</div>    \n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "YOUR REPO LINK GOES HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: Data and warm-up\n",
    "<hr>\n",
    "\n",
    "In this lab, you will build a variety of movie recommendation systems using the [MovieLens dataset](https://www.kaggle.com/prajitdatta/movielens-100k-dataset/data). The original source of the data is [here](https://grouplens.org/datasets/movielens/), and the structure of the data is described in the [README](http://files.grouplens.org/datasets/movielens/ml-latest-small-README.html) that comes with it. \n",
    "\n",
    "Run the code below which reads the ratings data as a CSV assuming that the file \"u.data\" is under `data/ml-100k/` directory in your lab folder. Timestamp can be useful in recommendation systems but we are going to ignore it in this assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_cols = [\"user_id\", \"movie_id\", \"rating\", \"timestamp\"]\n",
    "ratings = pd.read_csv(\n",
    "    os.path.join(\"data\", \"ml-100k\", \"u.data\"),\n",
    "    sep=\"\\t\",\n",
    "    names=r_cols,\n",
    "    encoding=\"latin-1\",\n",
    ")\n",
    "ratings = ratings.drop(columns=[\"timestamp\"])\n",
    "ratings.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll be using these keys later in the starter code\n",
    "user_key = \"user_id\"\n",
    "item_key = \"movie_id\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.1 Terminology\n",
    "Here is some notation we will be using in this lab. \n",
    "\n",
    "**Constants**:\n",
    "\n",
    " - $N$: the number of users, indexed by $n$\n",
    " - $M$: the number of movies, indexed by $m$\n",
    " - $\\mathcal{R}$: the set of indices $(n,m)$ where we have ratings in the utility matrix $Y$\n",
    "    - Thus $|\\mathcal{R}|$ is the total number of ratings\n",
    " - $k$: the number of latent dimensions we use in collaborative filtering\n",
    " \n",
    "**The data**:\n",
    "\n",
    " - $Y$: the utility matrix containing ratings, with a lot of missing entries\n",
    " - `train_mat` and `valid_mat`: Utility matrices for train and validation sets, respectively\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1 \n",
    "rubric={accuracy}\n",
    "\n",
    "**Your tasks:**    \n",
    "\n",
    "How many users and items are there in the movie ratings data?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1_1_1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N = ...\n",
    "M = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Number of users (N)  : {N}\")\n",
    "print(f\"Number of movies (M) : {M}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1.1.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2\n",
    "rubric={accuracy}\n",
    "\n",
    "**Your tasks:**    \n",
    "\n",
    "What would be the shape of the utility matrix $Y$? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1_1_2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# How many rows in the utility matrix?\n",
    "utility_n_rows = ...\n",
    "\n",
    "# How many columns in the utility matrix?\n",
    "utility_n_cols = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1.1.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3\n",
    "rubric={accuracy}\n",
    "\n",
    "**Your tasks:**    \n",
    "\n",
    "What is the percentage of non-nan ratings in the utility matrix $Y$? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1_1_3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "non_nan_ratings_percentage = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Non-nan ratings percentage: {np.round(non_nan_ratings_percentage,3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1.1.3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.4\n",
    "rubric={accuracy}\n",
    "\n",
    "**Your tasks:**    \n",
    "\n",
    "What are the average number of ratings per user and per movie? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1_1_4\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "avg_nratings_per_user = ...\n",
    "avg_nratings_per_movie = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Average number of ratings per user : {avg_nratings_per_user}\")\n",
    "print(f\"Average number of ratings per movie: {avg_nratings_per_movie}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1.1.4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data splitting \n",
    "rubric={accuracy}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Split the ratings data into train and validation splits with `test_size=0.2` and `random_state=42`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1_2_1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1.2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Utility matrix \n",
    "rubric={accuracy}\n",
    "\n",
    "The code below creates user and item mappers which map user ids and item ids to indices. \n",
    "\n",
    "**Your tasks:**\n",
    "1. Create utility matrices for train and validation sets and store them in `train_mat` and `valid_mat` variables, respectively. How many non-nan elements are there in each of these matrices? \n",
    "\n",
    "> You may use the code from lecture notes with appropriate attributions.  \n",
    "\n",
    "> You won't do it in real life but since our dataset is not that big, create a dense utility matrix in this assignment. You are welcome to try sparse matrix but then you may have to change some starter code provided in the later exercises.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_mapper = dict(zip(np.unique(ratings[user_key]), list(range(N))))\n",
    "item_mapper = dict(zip(np.unique(ratings[item_key]), list(range(M))))\n",
    "user_inverse_mapper = dict(zip(list(range(N)), np.unique(ratings[user_key])))\n",
    "item_inverse_mapper = dict(zip(list(range(M)), np.unique(ratings[item_key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mat = None\n",
    "valid_mat = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1_3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# What's the number of non-nan elements in train_mat (nnn_train_mat)?\n",
    "nnn_train_mat = ...\n",
    "\n",
    "# What's the number of non-nan elements in valid_mat (nnn_valid_mat)?\n",
    "nnn_valid_mat = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Number of non-nan elements in train_mat: {nnn_train_mat}\")\n",
    "print(f\"Number of non-nan elements in valid_mat: {nnn_valid_mat}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1.3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 1.4 Evaluation\n",
    "rubric={reasoning}\n",
    "\n",
    "You will be developing a number of models to complete the utility matrix in this assignment. To compare these models, you'll be evaluating them using the functions below. \n",
    "- Given two matrices, the `error` function below returns RMSE of non-nan elements.\n",
    "- Given predictions and train and validation utility matrices, the `evaluate` function below prints train and validation RMSEs by calling the `error` function for each set. \n",
    "\n",
    "**Your task:**\n",
    "\n",
    "1. Discuss this evaluation metric in the context of recommender systems focussing on the following points:\n",
    "    - Do we have ground truth in the context of recommender systems? \n",
    "    - What exactly are we comparing in order to evaluate recommender systems?\n",
    "    - Can we guarantee that the recommendations given by a systems with low RMSE are going to be effective recommendations in the sense that customers are likely to consume the recommended items? Briefly discuss. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def error(Y1, Y2):\n",
    "    \"\"\"\n",
    "    Given two matrices of the same shape, \n",
    "    returns the root mean squared error (RMSE).\n",
    "    \"\"\"\n",
    "    return np.sqrt(np.nanmean((Y1 - Y2) ** 2))\n",
    "\n",
    "\n",
    "def evaluate(pred_Y, train_mat, valid_mat, model_name=\"Global average\"):\n",
    "    \"\"\"\n",
    "    Given predicted utility matrix and train and validation utility matrices \n",
    "    print train and validation RMSEs.\n",
    "    \"\"\"\n",
    "    print(\"%s train RMSE: %0.2f\" % (model_name, error(pred_Y, train_mat)))\n",
    "    print(\"%s valid RMSE: %0.2f\" % (model_name, error(pred_Y, valid_mat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_1_4\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Baselines\n",
    "<hr>\n",
    "\n",
    "In this exercise you'll implement a number of baseline models to fill in the missing entries of the utility matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, the code below implements the global average rating baseline. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**global average rating baseline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = np.nanmean(train_mat)\n",
    "pred_g = np.zeros(train_mat.shape) + avg\n",
    "evaluate(pred_g, train_mat, valid_mat, model_name=\"Global average\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.1 Per-user average baseline\n",
    "rubric={accuracy}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Implement per-user average baseline and report train and validation RMSEs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2_1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "avg_n = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.2 Per-movie average baseline\n",
    "rubric={accuracy}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Implement per-movie average baseline and report train and validation RMSEs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2_2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "avg_m = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.3 Average of per-user and per-movie average baselines\n",
    "rubric={accuracy}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Implement average of per-movie and per-user averages baseline and report train and validation RMSEs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2_3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.4 $k$-nearest neighbours imputation\n",
    "rubric={accuracy}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Try [KNNImputer](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html) with at least 3 choices for the hyperparameter `n_neighbors` to fill in the missing entries. \n",
    "2. Report train and validation RMSEs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2_4\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 2.5 Discussion\n",
    "rubric={reasoning}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "Compare and discuss the results of all the baseline methods you tried in Exercise 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_2_5\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Collaborative filtering\n",
    "\n",
    "**Collaborative filtering** is one of the most popular approaches to fill in the missing entries of the utility matrix, which is based on something similar to LSA or `TruncatedSVD`.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 3.1 `TruncatedSVD` by replacing missing values with zeros\n",
    "rubric={accuracy,quality}\n",
    "\n",
    "Utility matrices are usually huge with many missing entries. If we want to use scikit-learn's `TruncatedSVD`, we first need to impute the missing values with some numeric values. In this exercise, you'll first center the non-nan ratings, replace missing entries with zeros, and experiment with `TruncatedSVD` with different values of $k$ (`n_components` hyperparameter of `TruncatedSVD`) to fill in the missing entries in the utility matrix. \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Subtract the averages of per user and per movie rating from non-nan ratings in the utility matrix. \n",
    "2. Replace missing values in the train utility matrix with zeros. \n",
    "> Hint: See help of [`np.nan_to_num`](https://numpy.org/doc/stable/reference/generated/numpy.nan_to_num.html). \n",
    "3. Train [`TruncatedSVD`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html) and get reconstructions to fill in the missing entries in the utility matrix. Experiment with at least a few values of $k$ (`n_components` hyperparameter of `TruncatedSVD`). Report train and validation RMSEs in each case. \n",
    "> When you reconstruct the data, do not forget to to add the averages you subtracted in the first step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_3_1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 3.2 Discussion \n",
    "rubric={reasoning}\n",
    "\n",
    "1. What's wrong with the approach in 3.1? Why is it not common to use `scikit-learn`'s `TruncatedSVD` for collaborative filtering? Why do we need a separate package?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_3_2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 3.3 Collaborative filtering with `surprise` package\n",
    "rubric={accuracy,reasoning}\n",
    "\n",
    "Use the [`surprise`](https://surprise.readthedocs.io/en/stable/) package which has implementation of dimensionality reduction with proper handling of missing values, which is suitable for recommendation systems. You can install it as follows in your environment. \n",
    "\n",
    "```\n",
    ">> conda activate 563\n",
    ">> conda install -c conda-forge scikit-surprise\n",
    "or \n",
    ">> pip install scikit-surprise\n",
    "```\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    " \n",
    "1. Carry out cross-validation using the [`surprise`](https://surprise.readthedocs.io/en/stable/) package and SVD algorithm. Report mean RMSEs. \n",
    "2. Briefly comment on the results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_3_3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4: Content-based recommenders\n",
    "<hr> \n",
    "\n",
    "Collaborative filtering is an unsupervised approach to fill in missing entries in the utility matrix. In this exercise, you'll explore content-based filtering, a supervised machine learning approach to recommendation systems. \n",
    "\n",
    "Collaborative filtering only uses the utility matrix. But usually there is information available about items (i.e., movies in our case) or users. Content-based filtering exploits this information along with ratings information to understand the taste of a user and predict their ratings for the items they have not consumed or rated yet. \n",
    "\n",
    "The code below loads movie genre features from `data/ml-100k/u.item` and stores them in a variable called `W`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [\n",
    "    \"movie_id\",\n",
    "    \"movie title\",\n",
    "    \"release date\",\n",
    "    \"video release date\",\n",
    "    \"IMDb URL\",\n",
    "    \"unknown\",\n",
    "    \"Action\",\n",
    "    \"Adventure\",\n",
    "    \"Animation\",\n",
    "    \"Children\",\n",
    "    \"Comedy\",\n",
    "    \"Crime\",\n",
    "    \"Documentary\",\n",
    "    \"Drama\",\n",
    "    \"Fantasy\",\n",
    "    \"Film-Noir\",\n",
    "    \"Horror\",\n",
    "    \"Musical\",\n",
    "    \"Mystery\",\n",
    "    \"Romance\",\n",
    "    \"Sci-Fi\",\n",
    "    \"Thriller\",\n",
    "    \"War\",\n",
    "    \"Western\",\n",
    "]\n",
    "\n",
    "movies_data = pd.read_csv(\n",
    "    os.path.join(\"data\", \"ml-100k\", \"u.item\"),\n",
    "    sep=\"|\",\n",
    "    names=cols,\n",
    "    encoding=\"latin-1\",\n",
    ")\n",
    "movies_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = [\n",
    "    \"Action\",\n",
    "    \"Adventure\",\n",
    "    \"Animation\",\n",
    "    \"Children\",\n",
    "    \"Comedy\",\n",
    "    \"Crime\",\n",
    "    \"Documentary\",\n",
    "    \"Drama\",\n",
    "    \"Fantasy\",\n",
    "    \"Film-Noir\",\n",
    "    \"Horror\",\n",
    "    \"Musical\",\n",
    "    \"Mystery\",\n",
    "    \"Romance\",\n",
    "    \"Sci-Fi\",\n",
    "    \"Thriller\",\n",
    "    \"War\",\n",
    "    \"Western\",\n",
    "]\n",
    "movie_genres = movies_data[genres]\n",
    "movie_genres.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = movie_genres.to_numpy()\n",
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Average number of genres per movie: {(W.sum() / M)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 4.1 Create `X` and `y` per user \n",
    "rubric={accuracy)\n",
    "\n",
    "In content-based filtering, we create a separate profile (`X` and `y`) for each user depending upon how many items they have rated so far. Since the number of items rated by each user is different, the size of `X` is going to be different for each user. The function `get_X_y_per_user` below creates `X` and `y` for each user with movie genre features.   \n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "1. Create `X` and `y` per user by calling the function `get_X_y_per_user` below on train and validation ratings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def get_X_y_per_user(ratings_df, d=W.shape[1]):\n",
    "    \"\"\"\n",
    "    Returns X and y for each user.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    ratings_df : pandas.DataFrame\n",
    "         ratings data as a dataframe\n",
    "\n",
    "    d : int\n",
    "        number of item features\n",
    "\n",
    "    Return:\n",
    "    ----------\n",
    "        dictionaries containing X and y for all users\n",
    "    \"\"\"\n",
    "    lr_y = defaultdict(list)\n",
    "    lr_X = defaultdict(list)\n",
    "\n",
    "    for index, val in ratings_df.iterrows():\n",
    "        n = user_mapper[val[user_key]]\n",
    "        m = item_mapper[val[item_key]]\n",
    "        lr_X[n].append(W[m])\n",
    "        lr_y[n].append(val[\"rating\"])\n",
    "\n",
    "    for n in lr_X:\n",
    "        lr_X[n] = np.array(lr_X[n])\n",
    "        lr_y[n] = np.array(lr_y[n])\n",
    "\n",
    "    return lr_X, lr_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_4_1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 4.2 Number of examples for each user\n",
    "rubric={accuracy,reasoning}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Write code to extract user ids with minimum and maximum number of examples in their corresponding `X`. Display user ids and the number of ratings available for these users. If there are multiple users with the same number of minimum or maximum number of ratings, just show the id and the number of ratings for one of them. \n",
    "2. Would the size of `X` have an impact on the recommendations given by a content-based recommender system for that user? Why or why not? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_4_2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 4.3 Training regression models per user\n",
    "rubric={accuracy}\n",
    "\n",
    "**Your tasks:**\n",
    "1. For each user, train regression models of your choice to predict missing ratings in the utility matrix. \n",
    "2. Report train and validation RMSEs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_4_3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### 4.4 Discussion \n",
    "rubric={reasoning}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Compare the validation RMSE from 4.3 with the validation RMSE you got with collaborative filtering. \n",
    "2. Discuss advantages and disadvantages of content-based filtering over collaborative filtering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_4_4\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 5: Food for thought\n",
    "<hr>\n",
    "\n",
    "Each lab will have a few challenging questions. These are usually low-risk questions and will contribute to maximum 5% of the lab grade. The main purpose here is to challenge yourself, dig deeper in a particular area, and going beyond what we explicitly discussed in the class. When you start working on labs, attempt all other questions before moving to these challenging questions. If you are running out of time, please skip the challenging questions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img/eva-game-on.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### (Challenging) 5.1 `top_n` predictions\n",
    "<hr>\n",
    "\n",
    "rubric={reasoning}\n",
    "\n",
    "**Your tasks:**\n",
    "1. Fit the SVD model on the train set using the `surprise` package. Write a function which returns `top_n` movie rating predictions for a given user id. Movies with these top ratings could be recommended to the user. \n",
    "\n",
    "> You may adapt [this code](https://github.com/NicolasHug/Surprise/blob/master/examples/top_n_recommendations.py) from the developer of the surprise package. If you do so, provide proper attributions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_5_1\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### (Challenging) Exercise 5.2 Hybrid models\n",
    "rubric={reasoning}\n",
    "\n",
    "When you will work as a data scientist, it is likely that you will be given a problem which needs to be solved without explicit instructions or scaffolding. Our hope is that the courses in MDS have taught you some fundamentals so that you at least know the right keywords to search for when you come across something that we have not explicitly talked about during the program. One of the useful skills to learn as a data scientist is to check whether there is a suitable tool available out there for your task. If yes, examining how useful and reliable it is and how easy/difficult it is to get it working.  \n",
    "\n",
    "In class, we noted that there are hybrid approaches for recommendation systems which combine collaborative filtering as well as content-based filtering.\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "Search on the internet and figure out whether there are any off-the-shelf tools or packages you can use to build hybrid recommendation systems. Try to get one of these packages working. Write a thoughtful paragraph on your experience with the package. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_5_2\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- BEGIN QUESTION -->\n",
    "\n",
    "### (Challenging) Exercise 5.3 Your takeaway from the course\n",
    "<hr>\n",
    "rubric={reasoning:1}\n",
    "\n",
    "**Your tasks:**\n",
    "\n",
    "What is your biggest takeaway from this course? Anything else you would like to share?\n",
    "\n",
    "> Detailed and thoughtful answers are appreciated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-warning\">\n",
    "\n",
    "Solution_5.3\n",
    "    \n",
    "</div>\n",
    "\n",
    "_Points:_ 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- END QUESTION -->\n",
    "\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before submitting your assignment, please make sure you have followed all the instructions in the Submission Instructions section at the top. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations on finishing the last lab of this course 🎉!! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(\"img/eva-congrats.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:563]",
   "language": "python",
   "name": "conda-env-563-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1.1.1": {
     "name": "q1.1.1",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert sha1(str(N).encode('utf8')).hexdigest() == 'cce637a738cef6fac143931ad866d768e84f2260', \"N is incorrect\"\n>>> assert sha1(str(M).encode('utf8')).hexdigest() == '544a5ef3b7270812304152163f0749ec895e665c', \"M is incorrect\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1.1.2": {
     "name": "q1.1.2",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert sha1(str(utility_n_rows).encode('utf8')).hexdigest() == 'cce637a738cef6fac143931ad866d768e84f2260', \"utility_n_rows is incorrect\"\n>>> assert sha1(str(utility_n_cols).encode('utf8')).hexdigest() == '544a5ef3b7270812304152163f0749ec895e665c', \"utility_n_cols is incorrect\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1.1.3": {
     "name": "q1.1.3",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert sha1(str(np.round(non_nan_ratings_percentage,3)).encode('utf8')).hexdigest() == 'eca68da8beec60feaf23e36b1f9098b1405d19b1', \"percentage of non-missing enties is incorrect\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1.1.4": {
     "name": "q1.1.4",
     "points": 2,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert sha1(str(int(avg_nratings_per_user)).encode('utf8')).hexdigest() == '7224f997fc148baa0b7f81c1eda6fcc3fd003db0', \"average number of ratings per user is incorrect\"\n>>> assert sha1(str(int(avg_nratings_per_movie)).encode('utf8')).hexdigest() == '5a5b0f9b7d3f8fc84c3cef8fd8efaaa6c70d75ab', \"average number of ratings per user is incorrect\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1.2": {
     "name": "q1.2",
     "points": 1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert sha1(str(X_train.shape[0]).encode('utf-8')).hexdigest() == '00bd44d0994ee0a71020743cee02292ed5c49ccb', \"Number of rows in X_train are incorrect\"\n>>> assert sha1(str(y_train.shape[0]).encode('utf-8')).hexdigest() == '00bd44d0994ee0a71020743cee02292ed5c49ccb', \"Number of rows in X_train are incorrect\"\n>>> assert sha1(str(X_valid.shape[0]).encode('utf-8')).hexdigest() == '352bc7d47decfa6b5052a0dd871ef73d6a91c7de', \"Number of rows in X_valid are incorrect\"\n>>> assert sha1(str(y_valid.shape[0]).encode('utf-8')).hexdigest() == '352bc7d47decfa6b5052a0dd871ef73d6a91c7de', \"Number of rows in X_valid are incorrecty\"\n>>> assert X_train.index[0] == 75220, \"Are you using the correct random state?\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q1.3": {
     "name": "q1.3",
     "points": 4,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert sha1(str(nnn_train_mat).encode('utf-8')).hexdigest() == '00bd44d0994ee0a71020743cee02292ed5c49ccb', \"The number of non-nan elements in train_mat is incorrect\"\n>>> assert sha1(str(nnn_valid_mat).encode('utf-8')).hexdigest() == '352bc7d47decfa6b5052a0dd871ef73d6a91c7de', \"The number of non-nan elements in valid_mat is incorrect\"\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
