{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](img/563_banner.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lecture 3: Introduction to Principal Component Analysis (PCA)\n",
    "\n",
    "UBC Master of Data Science program, 2021-22\n",
    "\n",
    "Instructor: Varada Kolhatkar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lecture plan and learning outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture plan \n",
    "\n",
    "- Introduction (~5 mins)\n",
    "- Summary of the pre-watch videos (~15 mins)\n",
    "- In-class activities and Q&A (~10 mins)\n",
    "- Break (~5 mins)\n",
    "- PCA applications (~30 mins)\n",
    "- Final comments, summary, and reflection (~10 mins)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(\"code/.\")\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import mglearn\n",
    "import seaborn as sns\n",
    "from IPython.display import display\n",
    "from ipywidgets import interact, interactive\n",
    "from plotting_functions import *\n",
    "from scipy.cluster.hierarchy import dendrogram, fcluster, linkage\n",
    "from sklearn import cluster, datasets, metrics\n",
    "from sklearn.cluster import DBSCAN, AgglomerativeClustering, KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from support_functions import *\n",
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "\n",
    "plt.rcParams[\"font.size\"] = 16\n",
    "# plt.style.use(\"seaborn\")\n",
    "%matplotlib inline\n",
    "pd.set_option(\"display.max_colwidth\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Learning outcomes <a name=\"lo\"></a>\n",
    "\n",
    "From this lecture, students are expected to be able to:\n",
    "\n",
    "- Explain some issues caused by high-dimensional data and the need for dimensionality reduction.\n",
    "- Explain the intuition behind Principal Component Analysis (PCA). \n",
    "- Describe the role and shapes of four matrices $X$, $W$, $Z$, and $\\hat{X}$ in the context of dimensionality reduction techniques;\n",
    "- Explain how to get $Z$ from $X$ and $W$. \n",
    "- Explain how to get $X_{hat}$ from $Z$ and $W$. \n",
    "- State the loss function for PCA. \n",
    "- Explain the difference between PCA and linear regression. \n",
    "    - Explain how PCA can be used in data compression, better representation, and visualization.  \n",
    "- Use `sklearn.decomposition.PCA` to perform Principal Component Analysis. \n",
    "- Use sklearn's `inverse_transform` to get reconstructions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dimensionality reduction: Motivation and introduction [[video](https://youtu.be/r-DwXpg1YDI)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Suppose you're shown the picture below and you are told that this is **Eva**. \n",
    "- Do you have to remember every pixel in the image to recognize other pictures of Eva? \n",
    "\n",
    "![](img/eva-tree.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- For example, if you are asked which one is Eva in the following pictures, it'll be fairly easy for you to identify her just based on some high-level features. \n",
    "\n",
    "![](img/hello-bmjs.png)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Just remembering important features such as shape of eyes, nose, mouth, shape and colour of hair etc. suffice to tell her apart from other people. \n",
    "- Can we learn such high-level features or **the summary** of the given raw features with machine learning models?\n",
    "- Yes! With dimensionality reduction techniques! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- As data scientists, given a dataset we either want to understand some phenomenon or build predictive models. \n",
    "- Very often the data we work with is clouded, complex, unclear, or even redundant.\n",
    "- But in reality the underlying phenomenon we are trying to understand or the relationship between variables in the data is much simpler. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Toy example: nutritional value of pizzas\n",
    "\n",
    "- Suppose we want to analyze nutritional value of pizzas of different brands. \n",
    "- Here is a [toy dataset](https://www.kaggle.com/shishir349/can-pizza-be-healthy) for this problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pizza_df = pd.read_csv(\"data/pizza.csv\")\n",
    "pizza_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_pizza = pizza_df.drop(columns=[\"id\", \"brand\"])\n",
    "y_pizza = pizza_df[\"brand\"]\n",
    "X_pizza.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pizza.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have features such as amount of moisture, amount of protein, amount of fat, amount of ash, amount of sodium, and amount of carbohydrates, and amount of calories per 100 grams in the sample.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's examine correlations between different variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_heatmat(X_pizza.corr(), w=6, h=3)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There is redundancy in the data; many features are correlated.  \n",
    "- Can we **summarize** these features in some meaningful way so that the data is cleaner and less redundant?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What do we mean by \"summarizing\" the data? \n",
    "\n",
    "- Can we just discard some redundant features? \n",
    "- We have seen some (not very satisfactory) feature selection methods to identify least important features in a greedy way and throw away such features.\n",
    "- This week we are going to look at a class of more sophisticated approaches for this, which are typically referred to as **dimensionality reduction**.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What is dimensionality reduction? \n",
    "\n",
    "**Dimensionality reduction** is the task of summarizing data or reducing a dataset in high dimension (e.g., 1000) to low dimension (e.g., 10) **while retaining the most \"important\" characteristics of the data.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Dimensionality reduction** is also used to reduce the dimensionality similar to feature selection. But\n",
    "- We will not be just dropping columns as we did in feature selection. \n",
    "- The idea of (linear) dimensionality reduction is to project high dimensional data to low dimensional space  while retaining the most \"important\" characteristics of the data. \n",
    "- We can also **reconstruct** the original data (with some error) from this transformed data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How do we reduce the dimensions?\n",
    "\n",
    "- The techniques we are going to look at this week summarize the data by creating new features which are **linear combinations of the original features**. \n",
    "- Example: \n",
    "$$\\text{new_feature} = 0.44 \\times fat + 0.47 \\times ash - 0.42 \\times carb \\dots$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dimensionality reduction toy example\n",
    "\n",
    "- Let's apply a popular dimensionality reduction technique called Principal Component Analysis (PCA) using [`sklearn`'s `PCA`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) on our nutritional value of pizzas toy example. \n",
    "- Learning a PCA model and transforming data is similar to applying preprocessing transformations in `sklearn`. \n",
    "- You can learn a PCA model and transform the data using `fit` and `transform` methods, respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "n_components = (\n",
    "    2  # summarize the data with only two features (typically called components)\n",
    ")\n",
    "pipe_pca = make_pipeline(\n",
    "    StandardScaler(), PCA(n_components=n_components)\n",
    ")  # scaling before PCA is crucial. We'll see the reason later.\n",
    "Z = pipe_pca.fit_transform(X_pizza)  # transform the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How does the data look like after dimensionality reduction? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "component_labels = [\"PCA\" + str(i + 1) for i in range(n_components)]\n",
    "pd.DataFrame(Z, columns=component_labels, index=X_pizza.index).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have reduced dimensionality from original 7 features to 2 features. \n",
    "- The two new features can be thought of as the **summary** of the original features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### What has it learned?\n",
    "\n",
    "- It has learned the \"most informative\" linear combinations of the features. \n",
    "- Each new feature (principal component) has a coefficient associated with each of the original features and the value of the new feature is a linear combination of the original features.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "W = pipe_pca.named_steps[\"pca\"].components_\n",
    "pd.DataFrame(W, columns=X_pizza.columns, index=component_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{PCA1} = 0.064709 \\times \\text{mois} + 0.378761 \\times \\text{prot} + \\dots + -0.424914 \\times \\text{carb} +  0.244487 \\times \\text{cal}$$\n",
    "\n",
    "$$\\text{PCA2} = -0.628276 \\times \\text{mois} + -0.628276 \\times \\text{prot} + \\dots + 0.320312 \\times \\text{carb} +  0.567458 \\times \\text{cal}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(Z[0, :], 4)  # transformed values for the 0th example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0_scaled = pipe_pca.named_steps[\"standardscaler\"].transform(X_pizza)[0, :]\n",
    "np.round((np.dot(x0_scaled, W[0, :]), np.dot(x0_scaled, W[1, :])), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_pca_w_vectors(W, component_labels, X_pizza.columns, width=800, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How good is the summary? \n",
    "\n",
    "- We can look at how much information from the original dataset these two newly created dimensions have captured. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipe_pca.named_steps[\"pca\"].explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are capturing 92.31% of the information using only two of these newly created features!!  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Let's look at how much \"information\" we can capture with different number of components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "n_components = len(X_pizza.columns)\n",
    "pipe_pca = make_pipeline(StandardScaler(), PCA(n_components=n_components))\n",
    "pipe_pca.fit(X_pizza)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    data=np.cumsum(pipe_pca[\"pca\"].explained_variance_ratio_),\n",
    "    columns=[\"variance_explained (%)\"],\n",
    "    index=range(1, n_components + 1),\n",
    ")\n",
    "df.index.name = \"n_components\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "simple_bar_plot(\n",
    "    x=df.index.tolist(),\n",
    "    y=df[\"variance_explained (%)\"],\n",
    "    x_title=\"n_components\",\n",
    "    y_title=\"variance explained (%)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "So the first two components summarize most of the information (92.31%) in the data!! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Common use cases for dimensionality reduction \n",
    "\n",
    "Overall this idea of summarizing the data in a meaningful way is going to be super useful and there are tons of applications for this. \n",
    "\n",
    "- **Data compression**    \n",
    "- **Feature extraction** in a machine learning pipeline \n",
    "    - Last week, we created PCA representation of face images before passing it to K-Means clustering. \n",
    "- **Visualization**\n",
    "    - Last week, we carried out dimensionality reduction using PCA to visualize our high dimensional data.\n",
    "- **Anomaly detection**\n",
    "- ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dimensionality reduction techniques \n",
    "\n",
    "We'll talk about the following linear dimensionality reduction techniques. \n",
    "\n",
    "- [Principal Component Analysis (PCA)](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) (in detail)\n",
    "- [TuncatedSVD or Latent Semantic Analysis (LSA)](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html) (brief discussion)\n",
    "- [Non-negative matrix factorization (NMF)](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html) (brief discussion)\n",
    "\n",
    "All these techniques can be viewed as applying transformations or \"change of basis\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Principal Component Analysis (PCA): Intuition and terminology [[video](https://youtu.be/33TRSSuzALw)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- PCA has been around for more than 100 years and is one of the most widely used dimensionality reduction techniques. \n",
    "- Examples:  \n",
    "    - [The Big Five personality traits](https://en.wikipedia.org/wiki/Big_Five_personality_traits) (extroversion, agreeableness, conscientiousness, neuroticism, openness to experience) were discovered using PCA. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Hockey-stick curve of global warming \n",
    "\n",
    "The famous hockey-stick curve of global warming was created by applying PCA on various temperature-related time series (tree rings, ice cores, etc.). [Source](https://www.wsj.com/articles/SB110834031507653590).\n",
    "\n",
    "![](img/climate-change-hockey.gif)\n",
    "\n",
    "<!-- <img src=\"img/climate-change-hockey.gif\" alt=\"\" height=\"300\",width=\"300\">  -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PCA intuition\n",
    "\n",
    "- PCA summarizes the data by finding linear combinations of features. \n",
    "- In fact, PCA finds the **best linear combinations** of the original features so that \n",
    "    - the first component has the most information \n",
    "    - the second component has the second most information \n",
    "    - and so on \n",
    "- What do we mean by finding the best linear combinations?    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Do we need two dimensions to describe these data points? \n",
    "\n",
    "- Let's create some synthetic data with two dimensions: $x_1$ and $x_2$   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "x1 = np.random.randn(10)\n",
    "x2 = 2 * x1\n",
    "X = np.stack([x1, x2], axis=1)\n",
    "plt.scatter(X[:, 0], X[:, 1], linewidths=4)\n",
    "plt.plot(x1, x2, c=\"k\", linewidth=0.5)\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"x2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data with some noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "x1 = np.random.randn(10)\n",
    "x2 = x1 + np.random.randn(10) / 3\n",
    "X = pd.DataFrame(data=np.stack([x1, x2], axis=1), columns=[\"x1\", \"x2\"])\n",
    "X_scaled = StandardScaler().fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(\n",
    "    X_scaled[:, 0], X_scaled[:, 1], c=X_scaled[:, 0], linewidths=3, cmap=\"viridis\"\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We are using colour just to keep track of different points. It doesn't particularly mean anything here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(X_scaled, columns=[\"x1\", \"x2\"])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature selection scenario\n",
    "\n",
    "- What would happen if we drop column x1? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_selection(data, 15, 6, drop=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All points are projected on the x2 axis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature selection scenario\n",
    "\n",
    "- What would happen if we drop column x2? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_selection(data, 15, 6, drop=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All points are projected on the x1 axis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PCA idea\n",
    "\n",
    "- How about finding an optimal line going through these points and projecting our data points on this line? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def f(angle):\n",
    "    plot_pca_model_search(data, alpha=angle, w=8, h=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "interactive(\n",
    "    f,\n",
    "    angle=widgets.IntSlider(min=0, max=180, step=1, value=0),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Reconstruction error: sum of the squared distances between original points and projected points.  \n",
    "- PCA picks the direction which gives the smallest **reconstruction error**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Interestingly, this is the same as **maximizes the variance of the projected points**.\n",
    "- So PCA learns a linear model (e.g., lines, planes, or hyperplanes) which minimizes the \"reconstruction error\" or maximizes the variance of the projected points.     \n",
    "- We'll look at the actual loss function later. \n",
    "- First, let's understand some terminology. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=1).fit(data)\n",
    "plot_pca_reconstructions(data, pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "n = 12\n",
    "d = 3\n",
    "\n",
    "x1 = np.linspace(0, 5, n) + np.random.randn(n) * 0.05\n",
    "x2 = -x1 * 0.1 + np.random.randn(n) * 2\n",
    "x3 = x1 * 0.7 + np.random.randn(n) * 3\n",
    "\n",
    "X = np.concatenate((x1[:, None], x2[:, None], x3[:, None]), axis=1)\n",
    "X = X - np.mean(X, axis=0)\n",
    "plot_interactive_3d(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "plot_2d_1k(X, pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PCA input/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "- Let's bring back our nutritional value of pizza dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pizza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pizza_scaled = StandardScaler().fit_transform(X_pizza)  # scale the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PCA input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "n_components = 2  # k = 2\n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(X_pizza_scaled);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A scaled matrix $X$ with $d$ dimensions (features) and $n$ examples\n",
    "- Number of components $k$\n",
    "   - We need to specify how many components we want to keep ($k$).\n",
    "- In our case, $n=300$, $d =7$, and $k=2$   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PCA output\n",
    "\n",
    "Two matrices: $Z$ and $W$\n",
    "\n",
    "- Projected data ($Z$)\n",
    "- Basis vectors ($W$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Projected data Z\n",
    "Z = pca.transform(X_pizza_scaled)  # transform the data\n",
    "component_labels = [\"PC\" + str(i + 1) for i in range(n_components)]\n",
    "pd.DataFrame(Z, columns=component_labels, index=X_pizza.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Basis vectors W\n",
    "W = pca.components_\n",
    "pd.DataFrame(W, columns=X_pizza.columns, index=component_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Output of PCA: transformed data $Z$ \n",
    "\n",
    "- Suppose the original data matrix $X$ has $n$ rows and $d$ columns, and we specify the number of components as $k$. \n",
    "- $Z$: Each row of $Z$ is a set of \"part weights\" of \"factor loadings\" or \"features\"\n",
    "    $$Z = \\begin{bmatrix}\n",
    "        z_{11} & \\ldots & z_{1k}\\\\ \n",
    "        z_{21} & \\ldots & z_{2k}\\\\ \n",
    "        & \\vdots &\\\\\n",
    "        z_{n1} & \\ldots & z_{nk}\n",
    "        \\end{bmatrix}_{n \\times k}\n",
    "    $$\n",
    "\n",
    "- It has $n$ rows and $k$ columns in contrast to $d$ columns in the original data (usually $k << d$). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Output of PCA: Basis vectors $W$ \n",
    "- $W$: Each row of $W$ is a set of **factors**, **principal components**, **parts**, or **basis vectors**. \n",
    "    $$W = \\begin{bmatrix}\n",
    "            w_{11} & \\ldots & w_{1d}\\\\ \n",
    "            w_{21} & \\ldots & z_{2d}\\\\ \n",
    "            & \\vdots &\\\\\n",
    "            w_{k1} & \\ldots & z_{kd}\n",
    "            \\end{bmatrix}_{k \\times d}\n",
    "    $$\n",
    "\n",
    "- We can access $W$ using `components_` attribute of the `PCA` object. \n",
    "- $W$ has $k$ rows, one for each component. \n",
    "- Each row has a coefficient or weight associated with all $d$ features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interpretation of the coefficients\n",
    "\n",
    "- You can interpret these coefficients similar to linear regression. \n",
    "    - Higher magnitude of the coefficient means the feature has a strong effect on the corresponding principal component. \n",
    "    - Positive coefficient means the feature and the principal component are positively correlated.\n",
    "    - Negative coefficient means the feature and the principal component are negatively correlated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_pca_w_vectors(W, component_labels, X_pizza.columns, width=800, height=800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reconstruction \n",
    "\n",
    "- In dimensionality reduction, unlike feature selection, we are not exactly throwing away features; we can reconstruct $X$ (with some error) by multiplying $Z$ and $W$ matrices. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(Z, columns=component_labels, index=X_pizza.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(W, columns=X_pizza.columns, index=component_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $\\hat{X}$ in the example above \n",
    "We can reconstruct examples by multiplying $Z$ and $W$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X_pizza_hat = Z @ W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_pizza_hat, columns=X_pizza.columns).round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reconstruction using `inverse_transform`\n",
    "- We can also access the reconstructed data using `inverse_transform` attribute of the PCA object. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_pizza_hat = pca.inverse_transform(Z)\n",
    "pd.DataFrame(X_pizza_hat, columns=X_pizza.columns).round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### More formally\n",
    "- We can get $\\hat{X}_{n \\times d}$ (reconstructed $X$) by matrix multiplication of $Z_{n \\times k}$ and $W_{k \\times d}$. \n",
    "$$\\hat{X}_{n \\times d} = Z_{n \\times k}W_{k \\times d} = \\begin{bmatrix}\n",
    "        z_{11} & \\ldots & z_{1k}\\\\ \n",
    "        z_{21} & \\ldots & z_{2k}\\\\ \n",
    "        & \\vdots &\\\\\n",
    "        z_{n1} & \\ldots & z_{nk}\n",
    "        \\end{bmatrix}_{n \\times k} \\times \n",
    "        \\begin{bmatrix}\n",
    "            w_{11} & \\ldots & w_{1d}\\\\ \n",
    "            w_{21} & \\ldots & w_{2d}\\\\ \n",
    "            & \\vdots &\\\\\n",
    "            w_{k1} & \\ldots & w_{kd}\n",
    "            \\end{bmatrix}_{k \\times d}$$\n",
    "- For instance, you can reconstruct an example $\\hat{x_{i}}$ as follows:  \n",
    "\n",
    "$$\\hat{x_{i}} = \\begin{bmatrix} z_{i1}w_{11} + z_{i2}w_{21} + \\dots + z_{ik}w_{k1} \\\\ z_{i1}w_{12} + z_{i2}w_{22} + \\dots + z_{ik}w_{k2}\\\\ \\vdots\\\\ z_{i1}w_{1d} + z_{i2}w_{2d} + \\dots + z_{ik}w_{kd}\\end{bmatrix}_{d \\times 1}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Reconstruction error\n",
    "\n",
    "- How good is the reconstructed data? \n",
    "- Are we able to accurately reconstruct the original data? \n",
    "- Let's compare our reconstructions to the original scaled data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_pizza_scaled).head(5)  # orginal scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_pizza_hat).head(5)  # reconstructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Let's calculate squared distances between original data and reconstructed data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def reconstruction_error(X, X_hat):\n",
    "    error = np.sum((np.array(X) - np.array(X_hat)) ** 2, axis=1)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_df = pd.DataFrame(reconstruction_error(X_pizza_scaled, X_pizza_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- As we can see, the reconstruction error is different for different examples. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One way to summarize these distances is by looking at the mean or median reconstruction error. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_df.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interim summary\n",
    "- Principal Component Analysis (PCA) is one of the widely used dimensionality reduction techniques. \n",
    "- The overall idea is to project high dimensional data onto a lower dimensional space to get a new representation. \n",
    "- It applies a linear transformation on the data and so it's a **linear dimensionality reduction** technique. \n",
    "- As input it takes number of components and scaled data and as output it results in two matrices: the transformed data matrix $Z$ and the weight matrix $W$.  \n",
    "- It's possible to reconstruct the original data (with some error) by multiplying $Z$ and $W$ matrices. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PCA loss function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the previous section we looked at\n",
    "- the intuition behind PCA\n",
    "- the input and output of PCA\n",
    "- how it approximates data matrix $X$ by matrix-matrix product $ZW$ or in other words how it approximates each example $x_i$ by the matrix-vector product $W^Tz_i$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- You will find a number of views and explanations for PCA.     \n",
    "- One way to view PCA is that it learns the hyperplane that minimizes the reconstruction error in the least squares sense. \n",
    "- Let's get an intuition for PCA loss function. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Let's generate some 2D data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "feat1 = np.random.randn(10)\n",
    "feat2 = feat1 + np.random.randn(10) / 3\n",
    "X = pd.DataFrame(data=np.stack([feat1, feat2], axis=1), columns=[\"feat1\", \"feat2\"])\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "data = pd.DataFrame(X_scaled, columns=[\"feat1\", \"feat2\"])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=1, random_state=42)\n",
    "pca.fit(data)\n",
    "Z = pca.transform(data)  # transformed data\n",
    "pd.DataFrame(Z, columns=[\"PC1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PCA reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hat = pca.inverse_transform(Z)\n",
    "pd.DataFrame(X_hat, columns=[\"recon_feat1\", \"recon_feat2\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's compare our reconstructions (`X_hat`) and original scaled data (`X_scaled`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plot_pca_model_reconstructions(data, pca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "- In our case $d = 2$ and $k = 1$. The green line corresponds to $W_{k\\times d}$ in the new 1D coordinate system.\n",
    "- `X_hat` are reconstructions. \n",
    "- PCA learns an optimal line, plane, or hyperplane so that reconstruction error is minimized. \n",
    "- Goal: Minimize the sum of squared distances between blue points (original) and red points (reconstructions).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PCA objective function\n",
    "\n",
    "In PCA we minimize the sum of squared error between elements of $X$ and elements of $ZW$: \n",
    "\n",
    "$$f(W,Z) = \\sum_{i=1}^{n} \\lVert{W^Tz_i - x_i}\\rVert^2_2 $$\n",
    "\n",
    "- $W^Tz_i \\rightarrow$ reconstructed example \n",
    "- $x_i \\rightarrow$ original example \n",
    "\n",
    "**Idea**: What are the best two matrices $W$ and $Z$ we can come up with so that when we multiply them we get a matrix that's closest to the original data.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PCA vs. linear regression\n",
    "\n",
    "- Minimizing squared error might remind you of linear regression.\n",
    "- **BUT THEY ARE NOT THE SAME.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- In case of linear regression, \n",
    "    - We minimize the squared error between true `y` and predicted `y`. \n",
    "    - We only care about the vertical distance because our goal is to predict `y` which we represent on the y-axis.\n",
    "- Unlike in regression we are also learning the features $z_i$ in PCA. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plot_pca_regression(data, 8, 6, error_type=\"both\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PCA algorithm \n",
    "\n",
    "- We want to find a transformation of the data such that we do not lose much information. In other words, \n",
    "    - we want the projections to be as close as possible to the original data\n",
    "- We can use optimization algorithms to find the solution. \n",
    "- But there is a better and faster way using linear algebra! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The standard PCA is as follows: \n",
    "\n",
    "- Center $X$ (subtract mean). (In practice, also scale to unit variance, i.e., apply `StandardScaler`.) \n",
    "- Compute **singular value decomposition (SVD)** of the data matrix to get principal components ($W$) and corresponding singular values which are associated with the **variance of each of the principal components**. \n",
    "- Drop principal components with smallest singular values for dimensionality reduction.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Singular Value Decomposition (SVD)\n",
    "\n",
    "- Singular value decomposition decomposes the given real-valued matrix $X_{n \\times d}$ into three matrices: \n",
    "\n",
    "$$X_{n \\times d} = U_{n \\times n}S_{n\\times d}V^T_{d \\times d}$$\n",
    "- $U_{n \\times n}$ is an orthogonal matrix (not necessarily calculated in practice)\n",
    "- $S_{n\\times d}$ is a diagonal matrix containing singular values, which correspond to the variance of each of the principal components.\n",
    "- $V^T_{d \\times d}$ is an orthogonal matrix which contains component vectors. \n",
    "    - For dimensionality reduction we drop rows of $V^T$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Another popular view of PCA is that it maximizes the **variance** of the projected (transformed) points. \n",
    "- We search for the direction of highest variance.  \n",
    "    - This direction is called the **first principal component**.     \n",
    "    - The next direction with highest variance is the **second principal component**.\n",
    "    - And so on ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look at an example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# source: Introduction to Machine Learning with Python book\n",
    "mglearn.plots.plot_pca_illustration()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "- Find the direction of maximum variance (Component 1). The direction with maximum variance contains most information in the data. \n",
    "- Find second direction which contains most information while being orthogonal (at right angle) to the first direction. (Component 2) \n",
    "- The head or tail of the arrows does not matter. They can point in any direction. \n",
    "- The directions found by this process are called **principal components**, as they are the directions of variance in the data. \n",
    "- There are usually as many components as the original features.\n",
    "- In dimensionality reduction, we consider the first $k<<d$ most important components. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- The top right plot shows the same data but rotated so that the first component aligns with the x-axis and the second component with the y-axis. \n",
    "- The mean is subtracted from the data before rotation so that the data is centered around zero. \n",
    "- In the bottom left plot, we are reducing dimensions from two dimensions to one dimensions. \n",
    "- We are keeping the most interesting direction with maximum variance (component 1). \n",
    "- In the bottom right plot we undo the rotation and add the mean back to the data. This is our reconstruction of the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### General idea \n",
    "\n",
    "- We want to find a transformation such that the transformed features are statistically uncorrelated. \n",
    "- We find new axes/basis vectors (rows of $W$) which are mutually orthogonal.\n",
    "- Each axis has eigenvalues which correspond to the variance in that direction and they decrease monotonically. \n",
    "- Since the eigenvalues decrease monotonically for our axes, once we are in the new space, we can drop the the axes with smallest eigenvalues for dimensionality reduction.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We want to find a transformation such that the transformed features are statistically uncorrelated. \n",
    "- We find new axes which are mutually orthogonal. \n",
    "- Line up the variance of the data along these axes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "tags": []
   },
   "source": [
    "### (Optional) Uniqueness of PCA solution\n",
    "\n",
    "- SVD gives us a solution with the following constraints to make it close to unique.     \n",
    "    - Normalization: we enforce that $\\lVert w_c \\rVert = 1$\n",
    "    - Orthogonality: we enforce that $\\lVert w_c \\rVert = 0$ for all $c \\neq c'$\n",
    "    - Sequential fitting: \n",
    "        - We first fit $w_1$ (\"first principal component\") giving a line.\n",
    "        - Then fit $w_2$ given $w_1$ (\"second principal component\") giving a plane.\n",
    "        - Then we fit $w_3$ given $w_1$ and $w_2$ (\"third principal component\") giving a hyperplane and so on \n",
    "\n",
    "Even with all this, the solution is only unique up to sign changes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ❓❓ Questions for you\n",
    "iClicker cloud join link: https://join.iclicker.com/MA16T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select all of the following statements which are **True** (iClicker)\n",
    "\n",
    "- (A) Each principal component of PCA is $d$ dimensional, where $d$ is the dimensionality of the original data.   \n",
    "- (B) You can think of transformed data $Z$ as the co-ordinates of $X$ in the new basis.\n",
    "- (C) When $k=d$, $Z$ will be exactly the same as $X$.  \n",
    "- (D) When $k=d$, $X_{hat}$ will be exactly the same as $X$.\n",
    "- (E) In PCA, it's best to pick $k$ as the largest value so that we capture most of the variance in the data.\n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``{admonition} Exercise 3.1 \n",
    "\n",
    "- (A) Each principal component of PCA is $d$ dimensional, where $d$ is the dimensionality of the original data.   \n",
    "- (B) You can think of transformed data $Z$ as the co-ordinates of $X$ in the new basis.\n",
    "- (C) When $k=d$, $Z$ will be exactly the same as $X$.  \n",
    "- (D) When $k=d$, $X_{hat}$ will be exactly the same as $X$.\n",
    "- (E) In PCA, it's best to pick $k$ as the largest value so that we capture most of the variance in the data.\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Exercise 3.1: V's Solutions!\n",
    ":class: tip, dropdown\n",
    "\n",
    "- (A) True\n",
    "- (B) True\n",
    "- (C) False\n",
    "- (D) True\n",
    "- (E) False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select all of the following statements which are **True** (iClicker)\n",
    "\n",
    "- (A) If you are reducing dimensionality from $d$ dimensions to $k$ dimensions using PCA, where $k\\leq d$, then your model is a $k$ dimensional hyperplane. \n",
    "- (B) In PCA, it's possible to select how many components you want to keep by looking at reconstructions.\n",
    "- (C) In PCA, it's possible to identify the most dominant features associated with each principal components.\n",
    "- (D) In PCA, the first principal component is always the one with highest variance in the data.\n",
    "- (E) Since PCA and linear regression both minimize squared error, PCA can be thought of as an unsupervised alternative for linear regression. \n",
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``{admonition} Exercise 3.2 \n",
    "\n",
    "- (A) If you are reducing dimensionality from $d$ dimensions to $k$ dimensions using PCA, where $k\\leq d$, then your model is a $k$ dimensional hyperplane. \n",
    "- (B) In PCA, it's possible to select how many components you want to keep by looking at reconstructions.\n",
    "- (C) In PCA, it's possible to identify the most dominant features associated with each principal components.\n",
    "- (D) In PCA, the first principal component is always the one with highest variance in the data.\n",
    "- (E) Since PCA and linear regression both minimize squared error, PCA can be thought of as an unsupervised alternative for linear regression. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{admonition} Exercise 3.2: V's Solutions!\n",
    ":class: tip, dropdown\n",
    "\n",
    "- (A) True\n",
    "- (B) True\n",
    "- (C) True\n",
    "- (D) True\n",
    "- (E) False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion questions\n",
    "- Why is PCA causing loss of information? Is it possible to use PCA without loss of information?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PCA applications "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "plt.rcParams[\"image.cmap\"] = \"gray\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Data \n",
    "\n",
    "- We'll be working with `sklearn`'s [Labeled Faces in the Wild dataset](https://scikit-learn.org/0.16/datasets/labeled_faces.html). \n",
    "- The dataset has images of celebrities from the early 2000s downloaded from the internet. \n",
    "\n",
    "> Credit: This example is based on the example from [here](https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch03.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "\n",
    "people = fetch_lfw_people(min_faces_per_person=20, resize=0.7)\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(10, 8), subplot_kw={\"xticks\": (), \"yticks\": ()})\n",
    "for target, image, ax in zip(people.target, people.images, axes.ravel()):\n",
    "    ax.imshow(image)\n",
    "    ax.set_title(people.target_names[target])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "people.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape = (87, 65)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are 3,023 images stored as arrays of 5655 pixels (87 by 65), belonging to 62 different people. \n",
    "- The data is skewed. Let's make the data less skewed by taking only 20 images of each person. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.zeros(people.target.shape, dtype=np.bool)\n",
    "for target in np.unique(people.target):\n",
    "    mask[np.where(people.target == target)[0][:20]] = 1\n",
    "\n",
    "X_people = people.data[mask]\n",
    "y_people = people.target[mask]\n",
    "# scale the grayscale values to be between 0 and 1\n",
    "# instead of 0 and 255 for better numeric stability\n",
    "X_people = X_people / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PCA for feature extraction\n",
    "\n",
    "- An important application of PCA is feature extraction. \n",
    "- Sometimes instead of using the raw data, it's useful to come up with a more interpretable and compact representation of the data. \n",
    "- For instance, in case of images, instead of looking at individual pixels, it's useful to look at important components. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Let's apply PCA on the data with n_components=100.\n",
    "- We'll look at how to pick `n_components` in the next lecture.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 100\n",
    "pca = PCA(n_components=100, random_state=123)\n",
    "pca.fit(X_people)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much variance are we covering with 100 components? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z = pca.transform(X_people)  # Transform the data\n",
    "W = pca.components_  # principal components\n",
    "X_hat = pca.inverse_transform(Z)  # reconstructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What will be the shape of $Z$, $W$, and $X_hat$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_people.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Z.shape  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data compression \n",
    "\n",
    "- One way to think of PCA is that it's a data compression algorithm. \n",
    "- If we store only $Z$ and $W$ instead of $X$, are we going to save space? Discuss with your neighbour.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Components learned by PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We won't quite be able to examine coefficients associated with all features and make sense of them because each feature in the original data just represents a pixel and there are a lot of them. \n",
    "- But we can show principal components as images and can look for semantic themes in them. \n",
    "- Let's examine the first few components learned by PCA as images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 5, figsize=(10, 6), subplot_kw={\"xticks\": (), \"yticks\": ()})\n",
    "for i, (component, ax) in enumerate(zip(W, axes.ravel())):\n",
    "    ax.imshow(component.reshape(image_shape), cmap=\"viridis\")\n",
    "    ax.set_title(\"{}. component\".format((i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The components encode some semantic aspects of images. \n",
    "- It's not always easy to interpret what semantic aspect each component is roughly capturing but we could make some guesses. \n",
    "- The first component is probably encoding the contrast between the background and the image. \n",
    "- The second component is probably encoding the differences in the lighting between left and right part of the image. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Original images vs. Reconstructed images \n",
    "\n",
    "- We can reconstruct the images from the transformed data $Z$ and components $W$. \n",
    "- Let's compare original images with the reconstructed images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_orig_reconstructed_faces(X_people[40:], X_hat[40:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decent reconstruction given that we are using only 100 components!! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### How many components? \n",
    "- We can decide this based on how much variance is covered by how many components (next lecture)\n",
    "- Or we can look at reconstruction of faces with varying number of components and pick $k$ based on your application. \n",
    "- Below we are reconstructing 3 faces with varying number of components in the range 10 to 500. \n",
    "- As we can see, with 100 components, we are already capturing recognizable face (compared to 7500 features in the original image). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "mglearn.plots.plot_pca_faces(X_people, X_people, image_shape)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Image lookup application\n",
    "\n",
    "- In social media and security applications a common task is to determine whether a new test face belongs to a face already in the database or not. \n",
    "- A reasonable way to solve this problem is by using 1-nearest neighbour. \n",
    "- Usually there are only a few examples of each person in the image databases and usual classification models where we consider each person as a separate class may not work very well.\n",
    "- Let's try $k$-NN with $k = 1$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Since we are working with a supervised learning application , let's split the data into train and validation splits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_people, y_people, stratify=y_people, random_state=0\n",
    ")\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $1$-NN with raw image representation\n",
    "\n",
    "- Let's try 1-NN on the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "pipe = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    KNeighborsClassifier(n_neighbors=1),\n",
    ")\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Train set accuracy of 1-nn: {:.2f}\".format(pipe.score(X_train, y_train)))\n",
    "print(\"Valid set accuracy: of 1-nn: {:.2f}\".format(pipe.score(X_valid, y_valid)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Considering that there are 62 classes, this score is not too bad as the baseline score would be pretty bad.  \n",
    "- Currently, we are looking at similarity between images by calculating distances between different pixels at particular locations.\n",
    "- This is not a great way to calculate similarity between images. \n",
    "- PCA would extract meaningful parts and probably could probably help here. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### $1$-NN with PCA representation  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_components = 90\n",
    "pipe_pca = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    PCA(n_components=n_components, whiten=True, random_state=123),\n",
    "    KNeighborsClassifier(n_neighbors=1),\n",
    ")\n",
    "\n",
    "pipe_pca.fit(X_train, y_train)\n",
    "print(\n",
    "    \"Variance Explained by %d principal components: %0.4f\"\n",
    "    % (n_components, sum(pipe_pca.named_steps[\"pca\"].explained_variance_ratio_) * 100)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Train/test results with PCA representation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Train set accuracy of PCA + 1-nn: {:.2f}\".format(pipe_pca.score(X_train, y_train))\n",
    ")\n",
    "print(\n",
    "    \"Valid set accuracy of PCA + 1-nn: {:.2f}\".format(pipe_pca.score(X_valid, y_valid))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Seems like we are overfitting in both cases but our validation results improved when we added PCA.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA for visualization\n",
    "\n",
    "- One of the most common applications of PCA is visualizing high dimensional data. \n",
    "- Suppose we want to visualize 20-dimensional [countries of the world data](https://www.kaggle.com/fernandol/countries-of-the-world). \n",
    "- The dataset has country names linked to population, area size, GDP, literacy percentage, birthrate, mortality, net migration etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/countries_of_the_world.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X = df.drop(columns=[\"Country\", \"Region\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's replace commas with periods in columns with type `object`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def convert_values(value):\n",
    "    value = str(value)\n",
    "    value = value.replace(\",\", \".\")\n",
    "    return float(value)\n",
    "\n",
    "\n",
    "for col in X.columns:\n",
    "    if X[col].dtype == object:\n",
    "        X[col] = X[col].apply(convert_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "- We have missing values\n",
    "- The features are in different scales. \n",
    "- Let's create a pipeline with `SimpleImputer` and `StandardScaler`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "n_components = 2\n",
    "pipe = make_pipeline(SimpleImputer(), StandardScaler(), PCA(n_components=n_components))\n",
    "pipe.fit(X)\n",
    "X_pca = pipe.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"Variance Explained by the first %d principal components: %0.3f percent\"\n",
    "    % (n_components, sum(pipe.named_steps[\"pca\"].explained_variance_ratio_) * 100)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The explained variance by the first two PCA components is $43.58\\%$.  \n",
    "- Good to know! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each example, let's get other information from the original data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "pca_df = pd.DataFrame(X_pca, columns=[\"pc1\", \"pc2\"], index=X.index)\n",
    "pca_df[\"Country\"] = df[\"Country\"]\n",
    "pca_df[\"Population\"] = X[\"Population\"]\n",
    "pca_df[\"GDP\"] = X[\"GDP ($ per capita)\"]\n",
    "pca_df[\"Crops\"] = X[\"Crops (%)\"]\n",
    "pca_df[\"Infant mortality\"] = X[\"Infant mortality (per 1000 births)\"]\n",
    "pca_df[\"Birthrate\"] = X[\"Birthrate\"]\n",
    "pca_df[\"Literacy\"] = X[\"Literacy (%)\"]\n",
    "pca_df[\"Net migration\"] = X[\"Net migration\"]\n",
    "pca_df.fillna(pca_df[\"GDP\"].mean(), inplace=True)\n",
    "pca_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "fig = px.scatter(\n",
    "    pca_df,\n",
    "    x=\"pc1\",\n",
    "    y=\"pc2\",\n",
    "    color=\"Country\",\n",
    "    size=\"GDP\",\n",
    "    hover_data=[\n",
    "        \"Population\",\n",
    "        \"Infant mortality\",\n",
    "        \"Literacy\",\n",
    "        \"Birthrate\",\n",
    "        \"Net migration\",\n",
    "    ],\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Interpreting `pc1` and `pc2`\n",
    "- Can we interpret the two dimensions?\n",
    "    - Recall that each row in $W$ matrix is a principal component. \n",
    "    - Each principal component has a coefficient associated with each feature in our original dataset. \n",
    "    - We can interpret the components by looking at the features with relatively bigger values (in magnitude) for coefficients for each components. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "component_labels = [\"PC \" + str(i + 1) for i in range(n_components)]\n",
    "W = pipe.named_steps[\"pca\"].components_\n",
    "plot_pca_w_vectors(W, component_labels, X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Dimensionality reduction to reduce overfitting in supervised setting\n",
    "\n",
    "- Often you would see dimensionality reduction being used as a preprocessing step in supervised learning setup. \n",
    "- More features means higher possibility of overfitting. \n",
    "- If we reduce number of dimensions, it may reduce overfitting and  computational complexity. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Dimensionality reduction for anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- A common application for dimensionality reduction is anomaly or outliers detection. For example:\n",
    "    - Detecting fraud transactions.  \n",
    "    - Detecting irregular activity in video frames.  \n",
    "    - It's hard to find good anomaly detection datasets. A popular one is [The KDD Cup ‘99 dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_kddcup99.html).     \n",
    "![](img/pca_anomaly_detection.png)    \n",
    "<!-- <img src=\"img/pca_anomaly_detection.png\" alt=\"\" height=\"900\" width=\"900\">  -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Final comments, summary, and reflection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Take-home message\n",
    "\n",
    "- **Dimensionality reduction** is the task of reducing a dataset in high dimension to low dimension **while retaining the most \"important\" characteristics of the data.** \n",
    "- PCA is one of the most widely used linear dimensionality reduction techniques. \n",
    "- Given data matrix $X_{n \\times d}$ and number of components $k \\leq d$, PCA outputs transformed data $Z_{n \\times k}$ and weight matrix $W_{k \\times d}$.\n",
    "- When going from higher dimensional space to lower dimensional space, PCA still tries to capture the topology of the points in high dimensional space, making sure  that we are not losing some of the important properties of the data. \n",
    "- So Points which are nearby in high dimensions are still nearby in low dimension. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- PCA reduces the dimensionality by learning a $k$-dimensional subspace of the original $d$-dimensional space.\n",
    "- To represent $k$-dimensional subspace we need $k$ basis vectors. \n",
    "    - Each basis vector or a principal component is $d$ dimensional. \n",
    "    - The basis vectors or principal components are the rows of $W$. \n",
    "        - So PCA learns $k$ basis vectors which define the transformations.        \n",
    "    - The representation in the new basis are the columns of $Z$.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- These principal components are basis vectors in the direction of maximum variance.\n",
    "- The basis vectors are orthogonal to each other. \n",
    "- Once we have $W$ we can obtain our transformed data points as a weighted sum of these components. \n",
    "    - $Z_{(n\\times k)} = X_{(n\\times d)}W^T_{(d\\times k)}$\n",
    "- We can also apply **inverse transformation** to recover $X$ from $Z$:\n",
    "    - $X_{(n\\times d)} \\approx Z_{(n\\times k)}W_{(k\\times d)}$ \n",
    "    - if $k=d$, then $\\approx$ becomes $=$ (i.e., you can precisely recover $X$)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In PCA, we minimize the squared error of reconstruction, i.e., elements of `X` and elements of `ZW`. \n",
    "- The goal is to find the two best matrices such that when we multiply them we get a matrix that's closest to the data. \n",
    "- A common way to learn PCA is using singular value decomposition (SVD). \n",
    "- When we apply SVD, we get the two best matrices $W=V^T$ and $Z=U\\Sigma$. \n",
    "- Although PCA and linear regression seem very similar cosmetically, they are two different algorithms. \n",
    "- We can access the variance explained by each component using `sklearn`'s `explained_variance_` and `explained_variance_ratio_`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PCA applications\n",
    "\n",
    "- PCA is useful in a number of applications. Some examples include\n",
    "    - Visualization \n",
    "    - Feature extraction\n",
    "    - Anomaly detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection (~4 mins)\n",
    "\n",
    "- PCA is a difficult concept to teach and to learn. \n",
    "- Go to this [Google doc](https://docs.google.com/document/d/1RXXXn7WOdU2uxKJv5eDAyZJ9DB1WMzucnBSVX0XpYJE/edit#heading=h.ylf9fhkpc6ik) and answer the following questions. \n",
    "    - What is your takeaway from this lesson? \n",
    "    - What concept from this lesson are you still struggling with? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Resources\n",
    "\n",
    "- [Introduction to Machine Learning with Python book Chapter 3](https://learning.oreilly.com/library/view/introduction-to-machine/9781449369880/ch03.html)\n",
    "- [PCA visualization](https://setosa.io/ev/principal-component-analysis/)\n",
    "- [StatQuest PCA video](https://www.youtube.com/watch?v=FgakZw6K1QQ&feature=youtu.be)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br><br><br>"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python [conda env:563]",
   "language": "python",
   "name": "conda-env-563-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
