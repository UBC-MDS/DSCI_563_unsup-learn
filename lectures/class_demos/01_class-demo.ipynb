{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4a5a1e4-cadb-4832-ae49-c4b519a86ce1",
   "metadata": {},
   "source": [
    "# Lecture 01: Clustering class demo\n",
    "\n",
    "![](../img/eva-fun-times.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86758193-4ebf-447f-8f5e-d7641f764f11",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Let's cluster images!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9d991b-c32c-44da-ac93-11238ce04d02",
   "metadata": {},
   "source": [
    "For this demo, I'm going to use two image datasets: \n",
    "1. A small subset of [200 Bird Species with 11,788 Images](https://www.kaggle.com/datasets/veeralakrishna/200-bird-species-with-11788-images) dataset (available [here](../data/birds.zip))\n",
    "2. A tiny subset of [Food-101](https://www.kaggle.com/datasets/kmader/food41?select=food_c101_n10099_r32x32x1.h5)\n",
    "(available [here](../data/food.zip))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfdeb43b-c67d-4a1f-bae0-64f8581104d9",
   "metadata": {},
   "source": [
    "To run the code below, you need to install pytorch and torchvision in the course conda environment. \n",
    "\n",
    "```conda install pytorch torchvision -c pytorch```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d3c219-d862-4b32-8854-cff703fd97b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "sys.path.append(os.path.join(os.path.abspath(\"..\"), \"code\"))\n",
    "from plotting_functions import *\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms, utils\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224f0b6a-d7db-4db0-81b5-21320884df17",
   "metadata": {},
   "source": [
    "Let's start with  small subset of birds dataset. You can experiment with a bigger dataset if you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfc808c-6616-4ace-8d90-76e863d09b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36e47a41-a219-4a23-9b14-0a1af951cc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0366dba9-0f2a-443d-9bcf-775afbff9f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e022916-14a1-401c-b6fd-76d2e001a141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "IMAGE_SIZE = 224\n",
    "def read_img_dataset(data_dir):     \n",
    "    data_transforms = transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),     \n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),            \n",
    "        ])\n",
    "               \n",
    "    image_dataset = datasets.ImageFolder(root=data_dir, transform=data_transforms)\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "         image_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0\n",
    "    )\n",
    "    dataset_size = len(image_dataset)\n",
    "    class_names = image_dataset.classes\n",
    "    inputs, classes = next(iter(dataloader))\n",
    "    return inputs, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5acd3a-c5b6-493a-ae4c-a03ad9a55c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_sample_imgs(inputs):\n",
    "    plt.figure(figsize=(10, 70)); plt.axis(\"off\"); plt.title(\"Sample Training Images\")\n",
    "    plt.imshow(np.transpose(utils.make_grid(inputs, padding=1, normalize=True),(1, 2, 0)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9284d9e8-8e63-4804-b767-09bd44bfeac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data/birds\"\n",
    "file_names = [image_file for image_file in glob.glob(data_dir + \"/*/*.jpg\")]\n",
    "n_images = len(file_names)\n",
    "BATCH_SIZE = n_images  # because our dataset is quite small\n",
    "birds_inputs, birds_classes = read_img_dataset(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd0dc45-fe6c-47fa-b1d9-e1f64f492891",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_birds = birds_inputs.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fae593-21eb-4483-bd32-c09a679a12b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample_imgs(birds_inputs[0:24,:,:,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb701e2-de98-485d-80d2-f0ab7ee35cfc",
   "metadata": {},
   "source": [
    "For clustering we need to calculate distances between points. So we need a vector representation for each data point. A simplest way to create a vector representation of an image is by flattening the image.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d4f256-dda2-4dce-ab4e-7bae90745703",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_transforms = transforms.Compose([    \n",
    "                    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),    \n",
    "                    transforms.Lambda(torch.flatten)])\n",
    "flatten_images = datasets.ImageFolder(root='../data/birds', transform=flatten_transforms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e6d339-6225-4f10-858b-06191c9275aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_dataloader = torch.utils.data.DataLoader(\n",
    "        flatten_images, batch_size=BATCH_SIZE, shuffle=True, num_workers=0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96cfd982-d309-495e-932d-c7bea40ad923",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_train, y_train = next(iter(flatten_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ada926-9142-4583-bd0f-f808e6e56654",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_images = flatten_train.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18551a5-ea53-473b-b634-cb14ce252998",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape=[3,224,224]\n",
    "img = flatten_images[20].reshape(image_shape)\n",
    "plt.imshow(np.transpose(img / 2 + 0.5, (1, 2, 0)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3231e93d-f886-4cd7-a070-bb10aca89c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_images.shape # 224 by 224 images with 3 color channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f134de-f351-4f34-81d4-e9b756286bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "k = 3\n",
    "km_flatten = KMeans(k, n_init='auto', random_state=123)\n",
    "km_flatten.fit(flatten_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5436737f-9944-48c6-8ee5-aa0d0ab65876",
   "metadata": {},
   "outputs": [],
   "source": [
    "km_flatten.cluster_centers_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50070e2-b06e-493a-a479-46031993efeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c11ebf3-96a6-43ee-a2fa-a2e6c69ec2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "unflatten_inputs = np.array([img.reshape(image_shape) for img in flatten_images])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd9f3d9-53f4-40e6-babf-3d88935d7533",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in range(k):\n",
    "    # user-defined functions defined in ../code/plotting_functions.py\n",
    "    get_cluster_images(km_flatten, flatten_images, unflatten_inputs, cluster, n_img=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd81eccd-e343-4cf1-810c-b9f032698e0b",
   "metadata": {},
   "source": [
    "Let's try clustering with GMMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19919c6e-fa5f-4a6e-ac93-453d14347969",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "gmm_flatten = GaussianMixture(n_components=k,covariance_type='diag', random_state=123)\n",
    "gmm_flatten.fit(flatten_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ebdfc9-8725-4258-9081-df8dad0f064f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in range(k):\n",
    "    # user-defined functions defined in ../code/plotting_functions.py\n",
    "    get_cluster_images(gmm_flatten, flatten_images, unflatten_inputs, cluster=cluster, n_img=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375786cd-df6d-424c-87d7-6cc23ffab8d3",
   "metadata": {},
   "source": [
    "We still see some mis-categorizations. It seems like when we flatten images, clustering doesn't seem that great.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb44483f-10d3-472a-8b96-c7f558d46366",
   "metadata": {},
   "source": [
    "Let's try out a different input representation. Let's use transfer learning as a feature extractor with a pre-trained vision model. For each image in our dataset we'll pass it through a pretrained network and get a representation from the last layer, before the classification layer given by the pre-trained network. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881a5855-92dd-4b05-9b4b-14241e0fd648",
   "metadata": {},
   "source": [
    "We see some mis-categorizations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35a1580b-08ff-4ed9-b49e-269900fba3e3",
   "metadata": {},
   "source": [
    "How about trying out a different input representation? Let's use transfer learning as a feature extractor with a pre-trained vision model. For each image in our dataset we'll pass it through a pretrained network and get a representation from the last layer, before the classification layer given by the pre-trained network. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8ada4a-3f99-487b-9d58-d070c7dda274",
   "metadata": {},
   "source": [
    "![](../img/cnn-ex.png)\n",
    "\n",
    "Source: https://cezannec.github.io/Convolutional_Neural_Networks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee30b46d-f982-4003-bf24-0ff62de0b4d0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def get_features(model, inputs):\n",
    "    \"\"\"Extract output of densenet model\"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():  # turn off computational graph stuff        \n",
    "        Z = model(inputs).detach().numpy()         \n",
    "    return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800c79fc-09f0-44c8-817c-d245ba4dcfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "densenet = models.densenet121(weights=\"DenseNet121_Weights.IMAGENET1K_V1\")\n",
    "densenet.classifier = torch.nn.Identity()  # remove that last \"classification\" layer\n",
    "Z_birds = get_features(densenet, birds_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdb20b2-43db-4464-a801-86c8b29aa5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_birds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f8b896-0d73-4d67-85cc-ed402536c077",
   "metadata": {},
   "source": [
    "Do we get better clustering with this representation? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f27d04d-fd22-41db-8a15-a8e8bf207eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k = 3\n",
    "km = KMeans(n_clusters=k, n_init='auto', random_state=123)\n",
    "km.fit(Z_birds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c84fc2e-04f5-4d08-a8ef-7923ff990296",
   "metadata": {},
   "outputs": [],
   "source": [
    "km.cluster_centers_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6a72f6-3429-47c3-b437-b0da7d064a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in range(k):\n",
    "    # user-defined functions defined in ../code/plotting_functions.py\n",
    "    get_cluster_images(km, Z_birds, X_birds, cluster, n_img=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a817b16b-2404-4b5e-b88b-b0b9749e1738",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5fac585-1965-4bc3-a0c8-4b59aa755c3f",
   "metadata": {},
   "source": [
    "KMeans seems to be doing a good job. But cluster centers are not interpretable at all now. Let's try GMMs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60c972d5-f2fb-4fe8-9935-436b7203fe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm = GaussianMixture(n_components=k, random_state=123)\n",
    "gmm.fit(Z_birds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8001308-82a6-40b8-9ffe-479638917c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm.weights_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28542b36-afd2-4e72-9a5b-8632917378be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in range(k):\n",
    "    # user-defined functions defined in ../code/plotting_functions.py\n",
    "    get_cluster_images(gmm, Z_birds, X_birds, cluster, n_img=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7e6a19-6c30-4d1a-9e57-dcf4016ddb3b",
   "metadata": {},
   "source": [
    "Cool! Both models are doing a great job with this representation!! This dataset seems easier, as the birds have very distinct colors. Let's try a bit more complicated dataset.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e7308c-dd20-4484-b670-4728e6255c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data/food\"\n",
    "file_names = [image_file for image_file in glob.glob(data_dir + \"/*/*.jpg\")]\n",
    "n_images = len(file_names)\n",
    "BATCH_SIZE = n_images  # because our dataset is quite small\n",
    "food_inputs, food_classes = read_img_dataset(data_dir)\n",
    "n_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf57c7c-5995-4945-b3d9-661119a54e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_food = food_inputs.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71426eae-8699-40b1-a0ce-19a62a0568d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sample_imgs(food_inputs[0:24,:,:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f712b3d-df87-4087-9761-f6883281c268",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_food = get_features(\n",
    "    densenet, food_inputs, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68d602dd-4660-46ce-9e57-6f9b1732b39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Z_food.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e20d9c-fd4e-4716-b8d6-3e49b159e454",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "k = 5\n",
    "km = KMeans(n_clusters=k, n_init='auto', random_state=123)\n",
    "km.fit(Z_food)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed229422-7b16-42f4-ab23-f2d1621fdd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "km.cluster_centers_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246caf2f-c3d8-474b-b24e-6c3310a11e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster in range(k):\n",
    "    get_cluster_images(km, Z_food, X_food, cluster, n_img=6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0cdb44-2a52-438d-9beb-34b23541607d",
   "metadata": {},
   "source": [
    "There are some mis-classifications but overall it seems pretty good! You can experiment with \n",
    "- Different values for number of clusters\n",
    "- Different pre-trained models\n",
    "- Other possible representations \n",
    "- Different image datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d89dbee-278a-4147-a715-1ba8f96e22c0",
   "metadata": {},
   "source": [
    "See an example of using K-Means clustering on customer segmentation in [AppendixA](../AppendixA.ipynb)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:563]",
   "language": "python",
   "name": "conda-env-563-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
